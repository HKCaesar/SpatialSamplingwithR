# Cluster random sampling {#Cl}

With stratified random sampling with geographical strata and systematic random sampling the sampling units are well spread throughout the study area. In general this leads to an increase of the precision of the estimated mean (total). With large study areas the price to be paid for this is long travel times, so that less sampling units can be observed in a given survey period. In this situation it can be more efficient to select *spatial clusters* of sampling units. In cluster random sampling, once a cluster is selected, all sampling units in this cluster are observed. For this reason this design is also referred to as *single-stage* cluster random sampling. The clusters are not subsampled, as in two-stage random sampling (see hereafter). A popular cluster shape is a transect. The reason is that the individual sampling units of a transect can easily be located in the field, which was a big advantage in the pre-GPS era.

The implementation of cluster random sampling is not straightforward. I have seen many examples in the literature of an improper implementation of this sampling design. A proper selection technique is as follows [@gru06]. In the first step a `starting point (unit)' is selected, for instance by simple random sampling. Then the remaining units of the cluster to which the starting point belongs are identified by making use of the definition of the cluster. For instance, with clusters defined as E-W oriented transects, with a cluster spacing of 100 m, all points east and west of the starting point at a distance of 100 m, 200 m *et cetera* that fall inside the study area are selected. These two steps are repeated until the required number of clusters is selected.

A requirement of a valid selection method is that the same cluster is selected, regardless of which of its locations is used as a starting point. In the example above this is the case: regardless of which of the points on the transect is selected first, the final set of points selected is the same.

An example of an improper implementation of this sampling design is the following. A cluster is defined as an E-W oriented transect of four points with a mutual spacing of 100 m. A cluster is selected by randomly selecting a starting point. The remaining three points of the cluster are selected E of this starting point. Points outside the study area are ignored. With this selection method the set of selected points is *not* independent of the starting point, and therefore is invalid.

Note that in this example the size of the clusters, i.e. the number of points (units) of a cluster is not constant but varies. With the selection method described above the selection probability of a cluster is proportional to its size (pps-sampling). With irregularly shaped study areas the size of the cluster can vary strongly. The size of the clusters can be controlled by subdividing the study area into blocks, for instance stripes perpendicular to the direction of the transects, or square blocks in case the clusters are grids. In this case, the remaining units are identified by extending the transect or grid until the boundary of the *block*. With irregularly shaped areas blocking will not eliminate entirely the variation in cluster sizes.

The ```R``` code below shows the selection of E-W oriented transects in Voorst. In order to delimit the length of the transects the study area is split into six 1 km $\times$ 1 km blocks. As a first step in the `R` script all clusters in the population are constructed. The cluster-id is added to the sampling frame. Each point belongs exactly to one cluster. Then several clusters are selected with probabilities proportional to size and with replacement (ppswr). This is done by simple random sampling with replacement of points, and identifying the clusters to which these points belong. Finally all points of the selected clusters are included in the sample.

```{r SampleCl}
s1local <- grdVoorst$s1-min(grdVoorst$s1)
s2local <- grdVoorst$s2-min(grdVoorst$s2)
#now define function for cluster random sampling
cl <- function(sframe,c,spacing) {
  #define clusters: E-W oriented transects
  s1f <- as.factor(s1local%%spacing)
  s2f <- as.factor(s2local)
  sframe$cluster <- as.character(interaction(s1f,s2f,as.factor(sframe$zone)))
  ids.pnts<-sample.int(nrow(sframe),size=c,replace=TRUE)
  ids.clusters <- sframe$cluster[ids.pnts]
  mysample <- sframe[sframe$cluster %in% ids.clusters,]
  return(mysample)
}
#set number of cluster draws
nClusters<-4 
#set spacing of points within clusters
clusterSpacing <- 100 
# set random seed (for reproduction of results)
set.seed(314)
mysample <- cl(sframe=grdVoorst,c=nClusters,spacing=clusterSpacing)
clusterMeans<-tapply(mysample$z,INDEX=mysample$cluster,FUN=mean)
estimatedMean<-mean(clusterMeans)
estimatedVarMean<-var(clusterMeans)/nClusters
```

Figure \@ref(fig:ClVoorst) shows the selected sample. The total number of selected points equals `r nrow(mysample)`. Similar to systematic random sampling, with cluster random sampling  the total sample size is random, so that we do not have perfect control of the total sample size. This is because in this case the sizes (number of points) of the clusters is not constant but varies.

```{r ClVoorst,echo=F,out.width='100%', fig.asp=.333, fig.cap="Cluster random sample from Voorst"}
ggplot(grdVoorst) +        
  geom_raster(mapping = aes(x = s1/1000, y = s2/1000,fill=as.factor(zone))) +
  geom_point(data=mysample, mapping = aes(x = s1/1000, y = s2/1000), size = 1.5) +
  scale_x_continuous(name = "Easting (km)") +
  scale_y_continuous(name = "Northing (km)") +
  coord_fixed()+
  theme(legend.position = "none")
```

## Estimation of mean and its sampling variance
With pps-sampling with replacement of clusters an unbiased estimator of the mean is:
\begin{equation}
\hat{\bar{z}}_{\mathrm{Cl}}=
\frac{1}{d}\sum\limits_{i=1}^{d}\hat{\bar{z}}_{i} \;,
(\#eq:EstMeanCl)
\end{equation}
where $d$ is the number of times a cluster is drawn (number of draws) and $\hat{\bar{z}}_{i}$ is the sample mean of cluster $i$.

The sampling variance can be estimated by
\begin{equation}
\widehat{V}\!\left(\hat{\bar{z}}_{\mathrm{Cl}}\right)=\frac{\widehat{S^2}_{\mathrm{Cl}}}{d} \;,
(\#eq:VarEstMeanCl)
\end{equation}
where $\widehat{S^2}_{\mathrm{Cl}}$ is the estimated variance of *estimated* cluster means (between cluster variance)
\begin{equation}
\widehat{S^2}_{\mathrm{Cl}} = \frac{1}{d-1}\sum_{i=1}^{d}(\hat{\bar{z}}_{i}-\hat{\bar{z}}_{\mathrm{Cl}})^2 \;,
(\#eq:S2EstMeanCl)
\end{equation}

Note that the size of the clusters (number of locations) does not appear in these formulas. This simplicity is due to the fact that the clusters are selected with probabilities proportional to size. The effect of the cluster size on the variance is implicitly accounted for. To understand this, consider that larger clusters result in smaller variance among their means.

Figure \@ref(fig:SamplingDistributionCl) shows the sampling distributions of the estimated mean with cluster random sampling and simple random sampling, obtained by repeating the random sampling with each design and estimation 10,000 times. For simple random samples the size is equal to the expected sample size of the cluster random sampling design.

```{r SamplingDistributionCl, out.width='70%', fig.asp=.8, fig.cap="Sampling distribution of estimated mean with Cl and SI", echo=F, cache=T}
NUMBER_OF_SAMPLES <- 10000
meanz <- Vmeanz <- meanzSI <- sampleSizes <- numeric(length=NUMBER_OF_SAMPLES)

s1f <- as.factor(s1local%%clusterSpacing)
s2f <- as.factor(s2local)
grdVoorst$cluster <- as.character(interaction(s1f,s2f,as.factor(grdVoorst$zone)))

#compute size of clusters
clustersize <- tapply(grdVoorst$z,INDEX=grdVoorst$cluster,FUN=length)

#compute expected sample size
p <- clustersize/sum(clustersize)
mean.n <- round(nClusters*sum(p*clustersize),0)

SI <- function(sframe, n) {
    units <- sample.int(nrow(sframe), size = n, replace = FALSE)
    mysample <- sframe[units,]
    return(mysample)
}

for (i in 1:NUMBER_OF_SAMPLES) {
    mysample <- cl(sframe=grdVoorst,c=nClusters,spacing=clusterSpacing)
    clusterMeans<-tapply(mysample$z,INDEX=mysample$cluster,FUN=mean)
    meanz[i]<-mean(clusterMeans)
    Vmeanz[i]<-var(clusterMeans)/nClusters
    sampleSizes[i]<-nrow(mysample)
    mySIsample <- SI(grdVoorst,n=mean.n)
    meanzSI[i] <- mean(mySIsample$z)
}

estimates<-data.frame(meanz,meanzSI)
names(estimates)<-c("Cl","SI")
estimateslf<-melt(estimates)

ggplot(data = estimateslf) +
    geom_boxplot(aes(y=value,x=variable)) +
    geom_hline(yintercept=mean(grdVoorst$z),colour="red")+
    scale_x_discrete(name = "Sampling design") +
    scale_y_continuous(name = "Estimated mean SOM")

meanmean<-mean(meanz)
varmean<-var(meanz)
meanvarmean<-mean(Vmeanz)
varmeanSI <- var(meanzSI)
```

The variance of the `r NUMBER_OF_SAMPLES` means with cluster random sampling equals `r round(varmean,3)`.  This is considerably larger than with simple random sampling: `r round(varmeanSI,3)`. The large variance is cause by the strong spatial clustering of points. This may save travel time, although in this small study area this is possibly rather limited. For large study areas the saving of travel time in the field with cluster random sampling can be considerable. The average of the estimated variances with cluster random sampling equals `r round(meanvarmean,3)`. This indicates that the estimator of the variance, Equation \@ref(eq:VarEstMeanCl), is unbiased. Figure \@ref(fig:histsamplesizeCl) shows the sampling distribution of the sample size. The expected sample size equals `r round(mean.n,2)`. 

```{r histsamplesizeCl, fig.width=4,fig.height=4,fig.cap="Sampling distribution of sample size", echo=F}
ggplot() +
        geom_histogram(aes(x=sampleSizes),binwidth=1,colour="orange") +
        scale_y_continuous(name = "Frequency") +
        scale_x_continuous(name = "Sample size")
#summary(sampleSizes)
```
