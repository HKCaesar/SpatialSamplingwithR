[
["index.html", "Spatial sampling with R Foreword", " Spatial sampling with R Dick J. Brus 2018-06-27 Foreword To be written "],
["GeneralIntro.html", "Chapter 1 Introduction 1.1 Design-based versus model-based approach", " Chapter 1 Introduction This book is about sampling for spatial surveys. Survey is defined as `the gathering of a sample of data or opinions considered to be representative of a whole’. So survey is an inventory of a population by collecting data on a part of the population. Examples are a survey of the organic carbon stored in the soil of an area, of the water quality in a lake, of the wood volume in a forest, of the total annual yield of rice in a country, et cetera. So this book is about observational research, not about experiments. In experiments observations are done under controlled circumstances, think of an experiment on crop yields as a function of application rates of fertilizer. Several levels of fertilizer application rate are chosen and randomly assigned to experimental plots. In observational research factors that influence the study variable are not controlled. This implies that in observational research no conclusions can be drawn on causal relations. In general we cannot afford a census in which all units of the population are observed. Some units of the population are selected and properties of the study variable are observed (measured) on these selected units only. These observations are subsequently used to derive characteristics of the whole population. For instance, to estimate the wood volume in a forest, we cannot afford to measure the wood volume of every tree in the forest. Instead, some trees are selected, the wood volume of these trees is measured, and based on these measurements the total wood volume of all trees in the forest is estimated. The selection of several units from all units in the population is referred to as sampling. Note that in the literature sampling is also used for the collection of material (e.g. soil, water, plant tissue) from a selected population unit. The material is taken to the lab for measurements. In this book sampling refers to the selection of a subset of population units from the entire population. Two types of populations can be distinguished: discrete and continuous populations. Discrete populations consist of a set of discrete, natural objects, think of trees, agricultural fields, lakes et cetera. In continuous populations such discrete objects do not exist, think of the soil in an agricultural field, or the water in a lake. The total number of units in a discrete population is finite, so a discrete population is a finite population. These units naturally serve as basic sampling unit. However in continuous populations no discrete objects exist that can serve as sampling unit. We must define the sampling units, e.g. 1 km by 1 km squares, or points. The total number of sampling units in a continuous population can be finite (e.g. all disjoint 1 km by 1 km squares in an area) or infinite (e.g. all points in an area). In this book the populations of interest have a spatial dimension, i.e. spatial coordinates are assigned to the population units. In sampling of spatial populations one may account for the spatial coordinates of the units, but this is not strictly needed. Examples are sampling designs spreading out the units throughout the study area, hopefully leading to more precise estimates of the population mean or total, and designs selecting spatial clusters of samples, leading to a reduced travel time. The sample data are used to estimate characteristics of the whole population, e.g. the population mean or total, some percentile (e.g. the median or 90\\(^{th}\\) percentile), or even the entire frequency distribution. In surveys of spatial populations the aim can also be to make a map of the population. As we have observed only a (small) part of the population, we are uncertain about the estimates and map. By using statistical methods we can quantify how uncertain we are about these results. In decision making it can be important to take this uncertainty into account. An example is a survey of water quality. In Europe the concentration levels of nutrients are regulated in the European Waterframework Directive. To test whether the mean concentration of a nutrient complies with its standard, it is important to account for the uncertainty in the estimated mean. When the estimated mean is just below the standard, there is still a large probability that the population mean exceeds the standard. This example shows that it is important to distinguish computing descriptive statistics from characterizing the population using the sample data. For instance, we can compute the sample mean (average of the sample data) without error, but if we use this sample mean as an estimate of the population mean, there is certainly an error in this estimate. Many sampling methods are available. At the highest level one may distinguish random from non-random sampling methods. In random sampling a subset of population units is randomly selected from the population, using a random number generator. Examples of non-random sampling are convenience sampling e.g. along roads, arbitrary sampling i.e. sampling without a specific purpose in mind, and targeted sampling e.g. at sites suspected of soil pollution. In the literature the term random sampling is often used for arbitrary sampling, i.e sampling without a specific purpose in mind. To avoid confusion the term probability sampling is used for random sampling using a random number generator, so that for any unit in the population the probabilities of selecting that unit is known. 1.1 Design-based versus model-based approach Which sampling method, probability or non-probability sampling, is best largely depends on the aim of the survey, see Brus and Gruijter (1997) and de Gruijter et al. (2006). Broadly speaking the following aims can be distinguished: To estimate parameters (mean, total, proportion, percentile) for the population To estimate parameters (mean, total, proportion, percentile) for several subpopulations To map the study variable The choice between probability or non-probability sampling is closely connected with the choice between a design-based or model-based approach for sampling and statistical inference (estimation, hypothesis testing). Table 1.1: Statistical approaches for sampling and inference Approach Sampling Inference Design-based Probability sampling Design-based Model-based Probability sampling not required Model-based In the design-based approach units are selected by probability sampling (Table 1.1). Estimates are based on the selection probabilities of the sampling units as determined by the sampling design (design-based inference). No model is used in estimation. On the contrary, in a model-based approach a stochastic model is used in estimation, for instance a regression model. As the model already contains a random error term, probability sampling is not required in this approach, which opens up the possibility of optimized non-probability sampling. When the aim is to map the study variable, a model-based approach is the only option1 This implies that for this aim probability sampling is not required. For estimating (sub)population parameters in principle both approaches are suitable. The more subpopulations are distinguished, the more attractive a model-based approach becomes. If the units are selected by probability sampling, then estimates of the population parameters can be obtained by design-based or model-based inference. This flexibility can be attractive, for instance when the sample size is rather small for model building. When the sampling units are not selected by probability sampling, model-free, design-based estimation is impossible, and model-based estimation is the only option. Questions Suppose a reseacher selects a sample of points from a study area by throwing darts on a map depicting the study area. Is the resulting sample a probability sample? If not, why not? Suppose we have a population of \\(N\\) units, numbered \\(1 \\cdots N\\). Can you think of a simple but proper way of selecting a probability sample of \\(n\\) units? What is the probability of selecting any given unit? With mapping I mean prediction at the points of a very fine grid.↩ "],
["IntroProbabilitySampling.html", "Chapter 2 Probability sampling for estimating (sub)population parameters 2.1 Horvitz-Thompson estimator 2.2 Simulated population", " Chapter 2 Probability sampling for estimating (sub)population parameters To estimate population parameters like the mean or the total, probability sampling is most appropriate. Probability sampling is random sampling using a random number generator such that 1. All population units have a probability larger than zero of being selected. 2. The selection probability of each possible sample is known. If the selection probability of each possible is known, for any unit in the population the probability that this unit is selected can be calculated as the sum of the selection probabilities over all samples that contains this unit. The selection probability of an individual unit is referred to as the inclusion probability of that unit. A common misunderstanding is that with probability sampling the inclusion probabilities must be equal. With many sampling designs they are unequal. This is no problem as long as these probabilities are known. There are many ways of selecting population units by probability sampling. The following sampling designs are described and illustrated in this book: * simple random sampling * stratified random sampling * systematic random sampling * sampling with probabilities proportional to size (pps-sampling) * cluster random sampling * two-stage random sampling * balanced sampling 2.1 Horvitz-Thompson estimator For any probability sampling design the population total can be estimated as a weighted sum of the observations \\[\\begin{equation} \\hat{t}(z)_{\\text{HT}}=\\sum_{i=1}^n w_i z_i = \\sum_{i=1}^n \\frac{1}{\\pi_i}z_i \\;, \\tag{2.1} \\end{equation}\\] with \\(n\\) the sample size (number of selected units), \\(w_i\\) the weight attached to unit \\(i\\), \\(z_i\\) the observed study variable for unit \\(i\\), and \\(\\pi_i\\) the probability that unit \\(i\\) is included in the sample (in short, inclusion probability). This estimator is referred to as the Horvitz-Thompson estimator or \\(\\pi\\)-estimator. So the observations are multiplied by a weight \\(w_i\\) which equals the inverse of the inclusion probabilities. One may think of the weights as the number of units in the population represented by the selected units. So, in sampling with unequal probabilities, the larger this inclusion probability of a unit is, the fewer poplation units this unit represents, and the smaller the weight attached to the observation on this unit. The \\(\\pi\\)-estimator for the mean is simply the \\(\\pi\\)-estimator for the total, divided by the total number of units in the population, \\(N\\): \\[\\begin{equation} \\hat{\\bar{z}}=\\frac{1}{N} \\sum_{i=1}^n \\frac{1}{\\pi_i}z_i \\;. \\tag{2.2} \\end{equation}\\] 2.2 Simulated population The sampling designs are illustated with simulated populations (pseudo-populations). Figure 2.1 shows one of these simulated populations. The population consists of simulated soil organic matter (SOM) concentrations (in mass-percentages) in the topsoil in an area near Voorst (Netherlands). The field is simulated with data collected at 132 points in the past years by graduate students of Wageningen University. Besides the point data I used an existing soil map and a land use map in the simulation. I think the map is a realistic picture of SOM in the study area. Figure 2.1: Simulated soil organic matter concentration in Voorst Figure 2.2 shows that SOM is skewed to the right. Figure 2.2: Histogram of SOM in Voorst Summary statistics are: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.061 4.859 6.807 8.119 9.918 46.700 The standard deviation of SOM in the population equals 4.9. The simulated populations are taken as reality. This means that the true values are known everywhere, as if the study varable is observed without error everywhere in the area. So for Voorst we know SOM at any randomly selected location, as well as the population mean and total. So, for any sample selected from this simulated population, the SOM values for the selected sampling units are known, so that we can estimate the population mean or total from this sample. The estimated mean (total) can be compared with the population mean (total). The difference between these two is the sampling error in the estimated mean (total). "],
["SI.html", "Chapter 3 Simple random sampling 3.1 Horvitz-Thompson estimator 3.2 Sampling variance of estimated mean, total and proportion 3.3 Confidence interval estimates 3.4 Arbitrary (haphazard) sampling versus probability sampling", " Chapter 3 Simple random sampling Simple random sampling is the most basic form of probability sampling. There are two subtypes: 1. Simple random sampling with replacement (SIR). 2. Simple random sampling without replacement (SI). This distinction is irrelevant for infinite populations. In simple random sampling with replacement a population unit may be selected more than once. In R a simple random sample with or without replacement can be selected by the function sample.int from the base package. For instance, a simple random sample without replacement of 10 units from a population of 100 units labeled as 1,2, … ,100, can be selected by set.seed(314) sample.int(100,size=10,replace=FALSE) ## [1] 10 27 76 22 20 29 23 35 51 68 The number of units in the sample is referred to as the sample size (\\(n=10\\) in the chunk above). Use argument replace = TRUE to select a simple random sample with replacement (SIR). When the spatial population is infinite we do not have a list of all units in the population that can serve as a sampling frame. In this case we use a map showing the boundaries of the population as a sampling frame. The selection procedure is as follows: Determine the minimum and maximum \\(s_1\\) and \\(s_2\\) coordinates of the field (bounding box). Draw two independent (pseudo-)random coordinates \\(s_{1,\\mathrm{ran}}\\) and \\(s_{2,\\mathrm{ran}}\\). Use a point-in-polygon routine to determine wether \\((s_{1,\\mathrm{ran}}, s_{2,\\mathrm{ran}})\\) falls within the area. Repeat steps 2 and 3 until \\(n\\) locations are selected. This procedure is implemented in the function spsample of R package sp (Pebesma and Bivand 2005). Alternatively, we may discretize the study area by a very fine grid, make a list of all the grid nodes, and sample from this list as before (see next chunk). So in this case the infinite population is represented as a very large finite population. In the next chunk a simple random sample without replacement (SI) of size 40 is selected from Voorst. Note that the class of the R object grdVoorst is a data.frame, so the infinite population is represented by a large, finite population. n &lt;- 40 N&lt;-nrow(grdVoorst) set.seed(314) units &lt;- sample.int(N, size = n, replace = FALSE) mysample &lt;- grdVoorst[units,] Restricting the sampling locations to the nodes of a discretisation grid can be avoided as follows. The columns s1 and s2 in the data.frame grdVoorst are the spatial coordinates of the centres of gridcells of 25 m by 25 m. Now a simple random sample is selected in two stages. First n times a gridcell is selected by simple random sampling with replacement. Second, every time a gridcell is selected, one point is selected fully randomly within this gridcell. In the chunk below the second step of this selection procedure is implemented with function jitter. It adds random noise to the spatial coordinates of the centres of the selected gridcells, by drawing from a continuous uniform distribution unif(\\(-c,c\\)), with \\(c\\) half the sidelength of the square gridcells. With this selection procedure we respect that the population actually is infinite. set.seed(314) units &lt;- sample.int(N, size = n, replace = TRUE) mysample &lt;- grdVoorst[units,] cellsize &lt;- 25 mysample$s1 &lt;- jitter(mysample$s1,cellsize/2) mysample$s2 &lt;- jitter(mysample$s2,cellsize/2) The result is shown in Figure 3.1. Figure 3.1: Simple random sample without replacement of size 40 from Voorst Drop outs What to do with selected units that do not belong to target population, or cannot be observed for whatever reason (e.g. no permission)? In practice it may happen that inspection in the field shows that a selected sampling unit does not belong to the target population. For instance, in a soil survey the sampling location may happen to fall on a road or in a build-up area. Shifting this location to a nearby unit may lead to biased estimates of the population mean, i.e., a systematic error in the estimated mean. This can be avoided by discarding these units and to replace them by sampling units on a reserve list, selected in the same way, i.e., by the same type of sampling design. The order of sampling units in this list must be the order in which they are selected. Do not replace a deleted sampling unit by the nearest sampling unit from the reserve list, but by the first unit, not yet selected, from the reserve list. 3.1 Horvitz-Thompson estimator For simple random sampling with replacement from a finite population the probability that a unit is selected in one draw equals \\(1/N\\). With \\(n\\) draws the probability that a unit is included in the sample (inclusion probability) equals \\(n/N\\). It can be shown that for simple random sampling without replacement the inclusion probabilities are equal to those with simple random sampling with replacement (Lohr 1999). Substituting this in the \\(\\pi\\)-estimator for the total (Equation (2.1)) gives for simple random sampling (with or without replacement) \\[\\begin{equation} \\hat{t}_{\\text{SI}}(z)=\\frac{N}{n}\\sum_{i=1}^n z_i=N \\bar{z}_s \\;, \\tag{3.1} \\end{equation}\\] with \\(\\bar{z}_s\\) the (unweighted) sample mean. So for simple random sampling the \\(\\pi\\) estimator of the mean is the unweighted sample mean: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{SI}} = \\bar{z}_s = \\frac{1}{n}\\sum_{i=1}^n z_i \\;. \\tag{3.2} \\end{equation}\\] For infinite populations the total can be estimated by the (unweighted) sample mean multiplied by the area of the region of interest \\(A\\): \\[\\begin{equation} \\hat{t}_{\\text{SI}}(z)= \\frac{A}{n}\\sum_{i=1}^{n}z_{i} \\;. \\tag{3.3} \\end{equation}\\] Comparing this estimator with the estimator for the finite population total (Equation (2.1)) shows that the inclusion probability \\(n/N\\) in the latter estimator has been replaced by \\(n/A\\), which can be interpreted as the inclusion probability , i.e., the number of sampling units per unit of area, or shortly, the . The simulated population is now sampled 10000 times. For each sample the mean is estimated, as well as the variance of the estimated mean. How the variance is estimated, is explained hereafter in Section 3.2. Figure 3.2 shows a histogram of the 10000 estimated means. Figure 3.2: Sampling distribution of estimated mean with SI of size 40 If we would repeat the sampling an infinite number of times and make the width of the bins in the histogram infinitely small, then we obtain, after scaling so that the sum of the area under the curve equals 1, the sampling distribution of the estimated mean. Important summary statistics of this sampling distribution are: 1. Expectation (mean) 2. Variance, referred to as the sampling variance When the expectation equals the population mean, then the estimator is p-unbiased (design-unbiased). Do not confuse the population variance and the sampling variance. The population variance (spatial variance) is a population characteristic, whereas the sampling variance is a characteristic of a sampling strategy, i.e. a combination of a sampling design and an estimator. The sampling variance quantifies our uncertainty about the mean. The sampling variance can be manipulated by changing the sample size \\(n\\), the type of sampling design, and the estimator. This has no effect on the population variance. The average of the 10^{4} estimated means equals 8.12, so the difference with the true population means equals -0.002. The variance of the 10^{4} estimated means equals 0.6. Questions: Compare the histogram of the estimated means with the histogram of the 7528 simulated values in the population (Figure 2.2). Explain the differences. What happens with the spread in the histogram (variance of estimated means) when the sample size \\(n\\) is increased? Suppose we would repeat the sampling a billion number of times, what would happen with the difference between the average of the estimated means and the population mean? In some cases one is interested in the proportion of the population (study area) satisfying a given condition. Think for instance of the proportion of trees in a forest infected by some disease, the proportion of an area in which a soil pollutant exceeds some critical threshold, or the proportion of an area where habitat conditions are suitable for some endangered species. A proportion is defined as the spatial mean of an 0/1 indicator \\(y\\) with value 1 if the condition is satisfied, and 0 else. So for simple random sampling this proportion can be estimated by the same formula as for the mean (Equation (3.2)): \\[\\begin{equation} \\hat{p}_{\\text{SI}} = \\frac{1}{n}\\sum_{i=1}^n y_i \\;. \\tag{3.4} \\end{equation}\\] 3.2 Sampling variance of estimated mean, total and proportion For simple random sampling of an infinite population and simple random sampling with replacement (SIR) of a finite population the sampling variance of the estimated mean equals \\[\\begin{equation} V\\!\\left(\\hat{\\bar{z}}_{\\text{SIR}}\\right)=\\frac{S^{2}(z)}{n} \\;, \\tag{3.5} \\end{equation}\\] with \\(S^{2}(z)\\) the population variance, also referred to as the spatial variance. For finite populations this population variance is defined as \\[\\begin{equation} S^{2}(z)=\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\left(z_{i}-\\bar{z}\\right)^{2} \\;, \\tag{3.6} \\end{equation}\\] and for infinite populations as \\[\\begin{equation} S^{2}(z) = \\frac{1}{A} \\int \\limits_{\\mathbf{s} \\in \\mathcal{A}} \\left(z(\\mathbf{s})-\\bar{z}\\right)^2\\text{d}\\mathbf{s} \\;. \\tag{3.7} \\end{equation}\\] In practice we select only one sample, i.e. we do not repeat the sampling many times. Still it is possible to estimate the variance of the estimated means if we would repeat the sampling. In other words, we can estimate the sampling variance of the estimated mean from a single sample. We do so by estimating the population variance from the sample, and this estimate can be used on his turn to estimate the sampling variance of the estimated mean. For simple random sampling with replacement (SIR) from finite populations the sampling variance of the estimated mean can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{SIR}}\\right)=\\frac{\\widehat{S^2}(z)}{n}= \\frac{1}{n\\,(n-1)}\\sum\\limits_{i=1}^{n}\\left(z_{i}-\\hat{\\bar{z}}\\right)^{2} \\;, \\tag{3.8} \\end{equation}\\] with \\(\\widehat{S^2}(z)\\) the estimated population variance. This estimator can also be used for infinite populations. For simple random sampling without replacement (SI) from finite populations the sampling variance of the estimated mean can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{SI}}\\right)=\\left(1-\\frac{n}{N}\\right)\\frac{\\widehat{S^2}(z)}{n} \\;. \\tag{3.9} \\end{equation}\\] Questions Is the sampling variance for simple random sampling without replacement larger or smaller than for simple random sampling with replacement, given the sample size \\(n\\)? Explain your answer. What is the effect of the population size \\(N\\) on this difference? The term \\(1-\\frac{n}{N}\\) is referred to as the finite population correction (fpc). In the sampling experiment described above, the average of the 10^{4} estimated sampling variances equals 0.597. The true sampling variance equals 0.599. So the difference is very small, indicating that the estimator of the sampling variance (Equation (3.9)) is unbiased. Questions Above I computed the true sampling variance, i.e. the variance of the estimated means if we would repeat the sampling an infinite number of times. How can this true sampling variance be computed? In reality we cannot compute this true sampling variance. Why not? The sampling variance of an estimated total of a finite population can be estimated by multiplying the estimated variance of the estimated mean by \\(N^2\\). For simple random sampling without replacement this estimator thus equals \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{t}_{\\text{SI}}(z)\\right)=N^2 (1-\\frac{n}{N})\\frac{\\widehat{S^{2}}(z)}{n} \\;. \\tag{3.10} \\end{equation}\\] For simple random sampling with replacement (SIR) and simple random sampling of infinite populations the sampling variance of the estimated total can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{t}_{\\text{SIR}}(z)\\right)=A^2\\frac{\\widehat{S^{2}}(z)}{n} \\;. \\tag{3.11} \\end{equation}\\] The sampling variance of an estimated proportion \\(\\hat{p}\\) can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{p}_{\\text{SI}}\\right)=\\left( 1-\\frac{n}{N}\\right) \\frac{\\hat{p}_{\\text{SI}}(1-\\hat{p}_{\\text{SI}})}{n-1} \\;. \\tag{3.12} \\end{equation}\\] The numerator in this estimator is an estimate of the population variance of the indicator. Note that this estimated population variance is divided by \\(n-1\\), and not by \\(n\\) as in the estimator of the mean. 3.3 Confidence interval estimates A second way of expressing our uncertainty about the estimated total or mean (proportion) is to present not merely a single number, but an interval. The wider the interval, the more uncertain we are, and vice versa, the narrower the interval, the more confident we are about the estimate. To learn how to compute a confidence interval, I return to the sampling distribution of the estimated mean soil organic carbon concentration. Suppose we would like to compute the bounds of an interval \\([a,b]\\) such that 5% of the estimated means is smaller than \\(a\\), and 5% is larger than \\(b\\). To compute the lower bound \\(a\\) and upper bound \\(b\\) of this 90%-interval, we must specify the distribution function. When the distribution of the target variable \\(z\\) is approximately normal, then the sampling distribution of the estimated mean is also approximately normal, regardless of the sample size. The larger the sample size, the smaller the effect of the distribution of \\(z\\) on the sampling distribution of the estimated mean. For instance, even when the distribution of \\(z\\) is far from symmetric, then still the sampling distribution of the estimated mean is approximately normal if the sample size is large, say \\(n &gt; 100\\). This is the essence of the Central Limit Theorem. Above we already noticed that the sampling distribution is much less asymmetric than the histogram of the simulated values, and looks much more like a normal distribution. Assuming a normal distribution, the bounds of the 90%-interval are given by: \\[\\begin{equation} \\bar{z} \\pm u_{1-(0.10/2)}\\cdot \\sqrt{V\\!\\left(\\hat{\\bar{z}}\\right)} \\;, \\tag{3.13} \\end{equation}\\] where \\(u_{1-(0.10/2)}\\) is the \\(0.95\\) quantile of the standard normal distribution. Note that in this equation the population mean \\(\\bar{z}\\) and the sampling variance of the estimated mean \\(V\\!\\left(\\hat{\\bar{z}}\\right)\\) are used. These quantities are unknown in practice, and must be estimated from the sample. Usually the standard normal distribution is replaced by the Student \\(t\\) distribution, which is a bit wider than the standard normal distribution. In this way we account for the unknown population variance. This leads to the following bounds of the \\(100(\\alpha/2)\\%\\) confidence interval estimate of the mean: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{SI}} \\pm t^{(n-1)}_{1-\\alpha /2}\\cdot \\sqrt{\\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{SI}}\\right)} \\;, \\tag{3.14} \\end{equation}\\] where \\(t^{(n-1)}_{1-\\alpha /2}\\) is the \\((1-\\alpha /2)\\) quantile of the Student \\(t\\) distribution with \\((n-1)\\) degrees of freedom. The quantity \\((1-\\alpha)\\) is referred to as the confidence level. The larger the number of degrees of freedom \\((n-1)\\), the closer the Student \\(t\\) distribution is to the standard normal distribution. The interpretation and definition of a confidence interval is not straightforward. A common misinterpretation is that if the 90% confidence interval estimate of the mean equals \\([a,b]\\), then the probability that the population mean is in this interval equals 90%. This cannot be a correct interpretation, because the population mean is not a random variable, and consequently the probability that the population mean is in an interval does not exist. However, the estimated bounds of the confidence interval are random variables, because the estimated mean and also the estimated sampling variance varies between samples drawn with the sampling design, so it does make sense to attach a probability to this interval. Figure 3.3 shows the 90% confidence interval estimates of the mean for the first 100 simple random samples drawn above. Note that both the location and the width of the intervals differ between samples. For each sample I determined whether this interval covers the population mean. Figure 3.3: Estimated confidence intervals of the population mean Out of the 10^{4} samples, 1174 samples do not cover the population mean. 3.4 Arbitrary (haphazard) sampling versus probability sampling In publications it is commonly stated that the sampling units were selected (more or less) at random (within strata), without further specification of how the sampling units were precisely selected. In statistical inference, the sampling units are subsequently treated as if they were selected by (stratified) simple random sampling. I would like to stress here that the term random sampling is often used in the meaning of arbitrary or haphazard sampling which is not equivalent to probability sampling. With probability sampling all units in the population have a positive probability of being selected, and the inclusion probabilities are known for all units. It is highly questionable whether this also holds for arbitrary and haphazard sampling. In arbitrary and haphazard sampling the sampling units are not selected by a probability mechanism. So the selection probabilities of the sampling units and of combinations of sampling units are unknown. This makes design-based estimation (design-based statistical inference) impossible, as this is based on the inclusion probabilities as determined by the sampling design, see the section on the Horvitz-Thompson estimator. The only way of statistical analysis of samples selected arbitrarily or haphazardly is model-based, i.e. a model of the spatial variation must be assumed. Exercise (SI.R) Write an R script to select a simple random sample of size 100 from Voorst (data are in Voorst.RData). Use the selected sample to estimate the population mean of SOM and its standard error (SOM is in the column z of the dataframe). Compute the lower- and upperbound of the 95-percent confidence interval using the Student \\(t\\) distribution, and check whether the population mean SOM is covered by the interval. "],
["STSI.html", "Chapter 4 Stratified random sampling 4.1 Estimation of the mean and its sampling variance 4.2 Confidence interval estimate 4.3 Optimal stratification 4.4 Geographical stratification 4.5 Allocation of sample size to strata", " Chapter 4 Stratified random sampling In stratified random sampling the population is divided into subpopulations, for instance soil mapping units, areas with the same land use or land cover, administrative units et cetera. The subareas do not overlap, and their sum equals the entire population (study area). Within each subpopulation, referred to as a stratum, a probability sample is selected by any probability sampling design. If these probability samples are selected by simple random sampling, as described in the previous chapter, this leads to stratified simple random sampling (STSI). If sampling units were selected by systematic random sampling, then this leads to stratified systematic random sampling. This chapter is about stratified simple random sampling. Stratified simple random samples can be selected with R package stratification, function strata. Take care that the order of the numeric with the number of units to be selected per stratum (stratum sample sizes) is equal to the order of the factor with the strata in the data file. Stratified simple random sampling is illustrated with Voorst (Figure 4.1). In the data frame with simulated data there is a column stratum. These are combinations of soil type and land use, obtained by overlaying a soil map and a land use map. library(sampling) # compute total number of pixels per stratum and stratum weights (relative size) Nh&lt;-tapply(X=grdVoorst$stratum, INDEX = grdVoorst$stratum, FUN =length) wh&lt;-Nh/sum(Nh) #total sample size n &lt;- 40 # compute stratum sample sizes for proportional allocation nh &lt;- round(n * wh) #sum of stratum sample sizes is 41, we want 40, so we reduce largest stratum sample size by 1 nh[1] &lt;- nh[1] - 1 units&lt;-strata(grdVoorst,stratanames=&quot;stratum&quot;,size=nh[unique(grdVoorst$stratum)],method=&quot;srswr&quot;) mysample&lt;-getdata(grdVoorst,units) cellsize &lt;- 25 mysample$s1 &lt;- jitter(mysample$s1,cellsize/2) mysample$s2 &lt;- jitter(mysample$s2,cellsize/2) The 40 sampling units are apportioned proportionally to the size (area, number of pixels) of the strata. The larger a stratum, the more units are selected from this stratum. Figure 4.1: Stratified simple random sample of size 40 from Voorst 4.1 Estimation of the mean and its sampling variance With simple random sampling within strata, the estimator of the mean for simple random sampling (Equation (3.2)) is applied at the level of the strata. The estimated stratum means are then averaged, using the relative sizes (relative areas) of the strata as weights: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{STSI}}= \\sum\\limits_{h=1}^{H} w_{h}\\,\\hat{\\bar{z}}_{h} \\;, \\tag{4.1} \\end{equation}\\] where \\(H\\) is the number of strata, \\(w_{h}\\) are the relative sizes (areas) of the strata (stratum weights): \\(w_h = N_h/N\\), and \\(\\hat{\\bar{z}}_{h}\\) is the estimated mean of stratum \\(h\\) estimated by the sample mean for stratum \\(h\\). The same estimator is found when the Horvitz-Thompson estimator is worked out for stratified simple random sampling. With stratified simple random sampling and different sampling fractions for the strata the inclusion probabilities differ between the strata and equal \\(\\pi_{hi} = n_h/N_h\\) for all \\(i\\) in stratum \\(h\\). Inserting this in the Horvitz-Thompson-estimator of the mean (Equation (2.2)) gives \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{STSI}}= \\frac{1}{N}\\sum\\limits_{h=1}^{H}\\sum\\limits_{i=1}^{n_h} \\frac{z_{hi}}{\\pi_{hi}} = \\frac{1}{N}\\sum\\limits_{h=1}^{H} \\frac{N_h}{n_h}\\sum\\limits_{i=1}^{n_h} z_{hi} = \\sum\\limits_{h=1}^{H} w_{h}\\,\\hat{\\bar{z}}_{h} \\;. \\tag{4.2} \\end{equation}\\] The sampling variance of the estimated mean is estimated by first estimating the sampling variances of the estimated stratum means, and then pooling. Take care, for the sampling variance we must square the stratum weights: \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{STSI}}\\right)=\\sum\\limits_{h=1}^{H}w_{h}^{2}\\,\\widehat{V}\\!\\left(\\hat{\\bar{z}}_{h}\\right)\\;, \\tag{4.3} \\end{equation}\\] where \\(\\widehat{V}\\!\\left(\\hat{\\bar{z}}_{h}\\right)\\) is the estimated sampling variance of \\(\\hat{\\bar{z}}_{h}\\): \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{h}\\right)= \\frac{\\widehat{S^2}_h(z)}{n_h}\\;, \\tag{4.4} \\end{equation}\\] with \\(\\widehat{S^2}_h(z)\\) the estimated spatial variance of \\(z\\) within stratum \\(h\\): \\[\\begin{equation} \\widehat{S^2}_h(z)=\\frac{1}{n_h-1}\\sum\\limits_{i=1}^{n_h}\\left(z_{hi}-\\hat{\\bar{z}}_{h}\\right)^{2}\\;, \\tag{4.5} \\end{equation}\\] and \\(n_{h}\\) is the number of sampling locations in stratum \\(h\\). Note that the sampling variance is for infinite populations and simple random sampling with replacement from finite population. For without replacement the variances of the estimated stratum means must be multiplied by the fpc’s \\(1- (n_h/N_h)\\). For the stratified sample of Figure 4.1 the estimated mean equals 7.6 and the estimated sampling variance of this estimate equals 0.35. Why should we stratify? There can be two reasons: We are interested in the means (totals) per stratum . We want to increase the precision of the estimated mean (total) for the entire population. Figure 4.2 shows the sampling distributions of the estimated mean for stratified simple random sampling and simple random sampling, both of size 40, obtained by repeating the random sampling with each design and estimation 10^{4} times. Figure 4.2: Sampling distribution of estimated mean for STSI and SI of size 40 The sampling distributions of the estimated means with the two designs are not very different; with stratified random sampling the spread of the estimated means is somewhat smaller. The horizontal red line is at the population mean. The gain in precision due to the statification, referred to as the stratification effect, can be quantified by the variance with simple random sampling divided by the variance with stratified simple random sampling. So when this variance ratio is larger than 1, stratified simple random sampling is more precise than simple random sampling. For Voorst the stratification effect with proportional allocation equals 1.196. Stratified simple random sampling with proportional allocation (see Section 4.5 hereafter) is more precise than simple random sampling when (Lohr 1999) \\[\\begin{equation} SSB &gt; \\sum_{h=1}^H (1-\\frac{N_h}{N})S^2_h \\;, \\tag{4.6} \\end{equation}\\] with SSB the weighted sum-of-squares between the stratum-means: \\[\\begin{equation} SSB= \\sum_{h=1}^H N_h (\\bar{z}_h-\\bar{z})^2 \\;. \\tag{4.7} \\end{equation}\\] So the smaller the differences in the stratum-means and the larger the variances within the strata, the smaller the stratification effect will be. Figure 4.3 shows boxplots of SOM per stratum (soil-landuse combination). The stratum means are equal to 8.98, 5.29, 6.39, 9.25, 11.27. The stratum variances are 27.9, 3.4, 9.4, 18.8, 36.5. The rather small differences in stratum means, in combination with the large stratum variances explain the modest gain in precision realized by stratified simple random sampling compared to simple random sampling. Figure 4.3: Boxplots of SOM per landuse-soil combination 4.2 Confidence interval estimate The \\(100(1-\\alpha )\\)% confidence interval for \\(\\bar{z}\\) is given by: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{STSI}} \\pm t_{1-\\alpha /2}^{(df)}\\cdot \\sqrt{\\widehat{V}\\!\\left(\\hat{\\bar{y}}_{\\text{STSI}}\\right)} \\;, \\tag{4.8} \\end{equation}\\] where \\(t_{1-\\alpha /2}^{(df)}\\) is the \\((1-\\alpha /2)\\) quantile of the Student \\(t\\) distribution with \\(df\\) degrees of freedom. \\(df\\) can simply be taken as \\(n-H\\), as proposed by Lohr (1999). This is the number of the degrees of freedom if the spatial variances within the strata are equal. With unequal spatial variances within strata \\(df\\) can be approximated by Sattherwaites method (Nanthakumar and Selvavel 2004): \\[\\begin{equation} df \\approx \\frac {\\left(\\sum_{h=1}^H a_h^2 \\frac{\\widehat{S^2}_h(z)}{n_h}\\right)^2} {\\sum_{h=1}^H a_h^4 \\left(\\frac{\\widehat{S^2}_h(z)}{n_h}\\right)^2 \\frac {1}{n_h-1}} \\;. \\tag{4.9} \\end{equation}\\] Exercise (STSI.R) Write an R script to select a stratified simple random sample of size 40 from Voorst (data are in Voorst.RData). Use variable stratumin the dataframe as a stratification variable, and use proportional allocation. Take care that the total sample size is 40. Estimate the population mean and its standard error Compute the lower- and upperbound of the 95-percent confidence interval using the Student t distribution, and check whether the population mean SOM is covered by the interval 4.3 Optimal stratification When we have a quantitative covariate related to the study variable and known for all units in the population, optimal strata can be constructed with the cumrootf method (Cochran 1977). Sampling units with similar values for the stratification variable are grouped into a stratum. Optimal strata are computed as follows: 1. Compute a histogram of the stratification variable using a large number of bins. 2. Compute the square root of the histogram frequencies. 3. Cumulate the square root of the frequencies, i.e. compute \\(\\sqrt{f_1}\\), \\(\\sqrt{f_1} + \\sqrt{f_2}\\), \\(\\sqrt{f_1} + \\sqrt{f_2} + \\sqrt{f_3}\\), et cetera. 4. Divide the total of the square root of the frequencies by the number of strata, multiply this value by 1, 2, \\(\\cdots\\), H-1, and select the boundaries of the histogram bins closest to these values. This method is illustrated with a survey of soil salinity at a cotton research farm of about 80 ha in Khorezm, Uzbekistan (Akramkhanov, Brus, and Walvoort 2013). For this farm a covariate map is available, depicting the apparent electrical conductivity as measured by an electromagnetic induction device (EM38), see Figure 4.4. Figure 4.4: EM38 measurements at cotton research farm in Khorezm, Uzbekistan The optimal strata can be constructed with the R package stratification (Baillargeon and Rivest 2011). Take care that the data are sorted in ascending order by the columns used for optimal stratification, see help of function strata.cumrootf. library(stratification) grdUzbekistan &lt;- grdUzbekistan[order(grdUzbekistan$EM),] #set sample size. This has no effect on the stratification n&lt;-40 #set number of strata H&lt;-10 #compute optimal strata Nu&lt;-length(unique(grdUzbekistan$EM)) #number of classes used in computing the histogram nclass&lt;-min(H*100,Nu) optstrata&lt;-strata.cumrootf(x=grdUzbekistan$EM,n=n,Ls=H,nclass=nclass) bh &lt;- optstrata$bh #stratum boundaries grdUzbekistan$optstrata&lt;-optstrata$stratumID Stratum boundaries are threshold values of the stratification variable EM; these boundaries are equal to 25.2, 32.5, 38.9, 45.2, 52.1, 59.8, 69, 81, 99.2. Note that the number of stratum boundaries is one less than the number of strata. The resulting stratification is shown in Figure 4.5. Figure 4.5: Optimal strata obtained with cumrootf Exercise (STSIcumrootf.R) Write an R script to compute ten cumrootf strata for the cotton research farm in Uzbekistan using EM as stratification variable (data are in CottonFarmUzbekistan.RData). Select a stratified simple random sample of size 40 (use function strata of package sampling). Use the stratum sample sizes as computed by the function strata.cumrootf. What allocation is used for computing the stratum sample sizes? Estimate the population mean of ECe and its variance. Compute the true sampling variance of the mean for this sampling design (Hint: compute the population variances of ECe per stratum, and divide these by the stratum sample sizes). Compute the stratification effect (gain in precision) (Hint: compute the sampling variance for simple random sampling by computing the population variance of ECe and divide this by the total sample size). 4.4 Geographical stratification When no covariate is available, we may still decide to apply a geographical stratification. For instance, a square study area can be divided into \\(4 \\times 4\\) equally sized subsquares that are used as strata. When we select one or two points per subsquare, we avoid strong spatial clustering of the sampling points. The geographical stratification improves the spatial coverage. When the study variable is spatially structured, think for instance of a spatial trend, then geographical stratification will lead to more precise estimated means (smaller sampling variances). A simple method for constructing geographical strata is k-means clustering (Brus, Spätjens, and Gruijter 1999) . In this approach the study area is discretized by a large number of pixels (raster cells). These pixels are the objects that are clustered. The clustering variables are simply the x-coordinate and y-coordinate of the centres of the pixels. This method leads to compact geographical strata. The method is implemented in the R package spcosa (Walvoort, Brus, and Gruijter 2010). Figure 4.6 shows 20 compact geostrata of equal area2. In each geostratum two points are selected by simple random sampling. Note that the sampling points are well spread throughout the study area. library(spcosa) #Choose number of geostrata H&lt;-20 set.seed(31415) gridded(grdVoorst)&lt;-~s1+s2 mygeostrata &lt;- stratify(object = grdVoorst,nStrata = H,nTry = 1,equalArea=F) grdVoorst &lt;- as(grdVoorst,&quot;data.frame&quot;) #select two points per stratum by simple random sampling mysample&lt;-spsample(mygeostrata,n=2) save(mygeostrata,mysample,file=&quot;Geostrata_Voorst.RData&quot;) #plot(mygeostrata,mysample) Figure 4.6: Compact geostrata of equal size in Voorst Questions Why is it attractive to select at least two points per geostratum? The alternative to 20 geostrata and two points per geostratum is 40 geostrata and one point per geostratum. Which sampling strategy will be more precise? The geostrata in the figure above have equal area, which can be enforced by argument equalArea=TRUE. Why are equal areas attractive? Work out the estimator of the population mean for strata of equal size. Exercise (STSIgeostrata.R) Write an R script to construct 20 compact geographical strata of equal area for Farmsum. Read the shapefile farmsum using function readOGR of the add-on package rgdal. Select two locations per geostratum, using function spsample of package spcosa. Now construct 40 strata of equal area and randomly select one point per stratum. Questions If only one location per stratum is selected, the sampling variance can be approximated by the collapsed strata estimator. In this method pairs of strata are formed, and the two strata of a pair are joined. In each new stratum we now have two points. With an odd number of strata there will be one group of three strata and three points. The sample is then analyzed as if it were a random sample from the new collapsed strata. Suppose we group the strata on the basis of the measurements of the target variable. Do you think this is a proper way of grouping? How would you group the strata? Will the estimated sampling variance estimator be unbiased? If not, will it be overestimated or underestimated? Exercise (STSIgeostrata_composite.R) Laboratory costs for measuring the study variable can be saved by bulking the soil aliquots (composite sampling). There are two options: bulking all soil aliquots from the same stratum (bulking within strata) or bulking by selecting one aliquot from each stratum (bulking across strata). In spcosa bulking across strata is implemented. Write an R script to construct 20 compact geographical strata for study area Voorst. Use the argument equalArea = TRUE. Select four points per stratum using argument type=&quot;composite&quot;, and change class of resulting object in SpatialPoints Extract the z-values in grdVoorst at the selected sampling points using function %over%. Add a column to the resulting dataframe indicating the composite (points 1 to 4 are from the first stratum, points 5 to 8 from the second stratum et cetera), and estimate the means for the four composites using functin tapply. Estimate the population mean and its standard error. Questions Can the sampling variance of the estimated mean be estimated for bulking within the strata? The alternative is to analyze all \\(20 \\times 4\\) aliquots separately, instead of the four composites only. The strata have equal area, so the inclusion probabilities are equal. As a consequence the sample mean is an unbiased estimate of the population mean. Is the precision of this estimated population mean equal to the estimated population mean with composite sampling? If not, is it smaller or larger, and why? If you use argument equalArea = FALSE in combination with argument type=&quot;composite&quot;, you get an error message. Why does this not work? 4.5 Allocation of sample size to strata After we have decided on the total sample size \\(n\\), we must decide on how to apportion these to the strata. It is reasonable to allocate more sampling units to large strata, and fewer to small strata. This leads to proportional allocation: \\[\\begin{equation} n_{h}=n \\cdot \\frac{N_{h}}{\\sum N_{h}}\\;, \\tag{4.10} \\end{equation}\\] with \\(N_h\\) the total number of sampling units (size) of stratum \\(h\\). With infinite populations \\(N_h\\) is replaced by the area \\(A_h\\). Besides, if we have prior information on the variance of the study variable within the strata, then it makes sense to account for differences in spatial variance. Heterogeneous strata should receive more sampling units than homogeneous strata. Finally, costs of sampling may differ between strata. It can be relatively expensive to sample nearly inaccessible strata, and we do not want to sample many units there. This leads to optimal allocation: \\[\\begin{equation} n_{h}= n \\cdot \\frac{\\frac{N_{h}\\,S_{h}}{\\sqrt{c_{h}}}}{\\sum\\limits_{h=1}^{H} \\frac{N_{h}\\,S_{h}}{\\sqrt{c_{h}}}} \\;, \\tag{4.11} \\end{equation}\\] with \\(S_h\\) the spatial standard deviation (square root of spatial variance) of stratum \\(h\\), and \\(c_h\\) the costs per sampling unit in stratum \\(h\\). Optimal means that given the total costs this allocation type leads to minimum sampling variance (assuming a linear costs model). So the more variable a stratum and the lower the costs, the more units will be selected from this stratum. If we take equal costs per sampling unit for all strata, then optimal allocation reduces to so-called Neyman allocation: \\[\\begin{equation} n_{h}= n \\cdot \\frac{N_{h}\\,S_{h}}{\\sum\\limits_{h=1}^{H} N_{h}\\,S_{h}} \\;. \\tag{4.12} \\end{equation}\\] If the total number of pixels divided by the number of strata is an integer, the stratum sizes are exactly equal, otherwise the difference is 1 pixel.↩ "],
["SY.html", "Chapter 5 Systematic random sampling 5.1 Estimation of mean and its sampling variance", " Chapter 5 Systematic random sampling A simple way of drawing probability samples whose locations are spread uniformly over the study area, is systematic random sampling (SY). Figure 5.1 shows a square grid randomly placed on study area Voorst. set.seed(314) n &lt;- 40 #expected sample size #change class of grdVoorst to SpatialPixelsDataFrame gridded(grdVoorst) &lt;- ~s1+s2 mysample&lt;-spsample(x=grdVoorst,n=n,type=&quot;regular&quot;) mysample&lt;-as(mysample,&quot;data.frame&quot;) Figure 5.1: Systematic random sample from Voorst I used the argument n in function spsample to set the sample size. Note that this is the expected sample size, i.e. on average over repeated sampling the sample size is 40. In Figure 5.1 the number of selected sampling points equals 41. Alternative shapes for the grid cells are triangular and hexagonal. Triangular grids can be selected with function spsample with the argument type = hexagonal (which, I admit, is confusing). The triangular shape was shown to be most efficient in general. Besides the shape of the grid cells, we must decide on the size of the grid cells (grid spacing) and on the orientation of the grid. The grid spacing determines the number of sampling locations in the study area. So, if we have decided on the required (allowed) number of sampling locations, then we may use this number to calculate the grid spacing. For square grids, the grid spacing (m) can be calculated with \\(\\sqrt{A/n}\\), where \\(A\\) is the area in m\\(^2\\), and \\(n\\) is the number of sampling locations. Instead of argument n of function spsample we may use argument argument cellsize to select a grid with a specified spacing. The grid is randomly placed over the study area as follows. One location is selected by simple random sampling from the study area. Given the chosen orientation of the grid, the grid is extended in all directions using the selected location as a starting node. Finally, all nodes are selected that fall within the study area. There is no strict need for random selection of the orientation of the grid, random selection of the first node suffices for design-based statistical inference (estimation). In general, the spatial coverage with random grid sampling is better than with a stratified random sample using compact geographical strata, even with one location per geostratum. Consequently, in general systematic random sampling will give on average more precise estimates of the mean or total. There are two disadvantages of systematic random sampling compared to geographically stratified random sampling. First, for systematic random sampling no unbiased estimator of the sampling variance exists. This is because we do not have independent replicates of the sample: the grid basically is one ‘cluster’ of sampling locations. Second, the number of sampling locations with random grid sampling is not fixed, but varies between randomly drawn samples. We may choose the grid spacing such that on average the number of sampling locations equals the required (allowed) number of sampling locations, but for the actually drawn sample, this number can be several locations smaller or larger. When the sample size shows strong variation, this may worsen the quality of the estimated mean. In Voorst the variation of the sample size is very large when the orientation of the grid is parallel to the sides of the rectangular study area (Figure 5.2). The histogram shows a bimodal distribution. The smaller sample sizes are of square grids with only two rows of points instead of three rows. Figure 5.2: Sampling distribution of sample size of SY A large variation in sample size leads to a large sampling variance of the estimated mean, and therefore should be avoided. A random sample size may also be undesirable when this size is prescribed, for instance, in regulations. For voorst the variation in sample size can be reduced by changing the orientation. The R chunk below shows how to select a systematic random sample with a user-defined orientation. Figure 5.3 shows the selected square grid with orientation N60E. gridtop &lt;- as(getGridTopology(grdVoorst),&quot;data.frame&quot;) A &lt;- nrow(grdVoorst)*gridtop$cellsize[1]*gridtop$cellsize[2] spacing &lt;- sqrt(A/n) nx &lt;- floor((gridtop$cellsize[1]*gridtop$cells.dim[1])/spacing)+1 ny &lt;- floor((gridtop$cellsize[2]*gridtop$cells.dim[2])/spacing)+1 #choose angle angle&lt;- -pi/6 #construct square grid. Note large number of rows with negative y-coordinates x &lt;- seq(from=0,to=nx*2,by=1)*spacing y &lt;- seq(from=ny*-10,to=ny,by=1)*spacing xygrd &lt;- expand.grid(x,y) names(xygrd) &lt;- c(&quot;s1&quot;,&quot;s2&quot;) #compute coordinates on rotated axes xygrd$s1r &lt;- xygrd$s1*cos(angle)+xygrd$s2*sin(angle) xygrd$s2r &lt;- -xygrd$s1*sin(angle)+xygrd$s2*cos(angle) #select random start origin &lt;- gridtop$cellcentre.offset start &lt;- data.frame(s1=runif(1,0,spacing)+origin[1],s2=runif(1,0,spacing)+origin[2]) #add start to coordinates xygrd$s1rs &lt;- xygrd$s1r+start$s1 xygrd$s2rs &lt;- xygrd$s2r+start$s2 #intersect rotated grid with SpatialPixelsDataFrame to see which gridpoints are in study area coordinates(xygrd) &lt;- ~s1rs+s2rs res &lt;- (xygrd %over% grdVoorst) mysample &lt;- xygrd[!is.na(res$z),] mysample$z &lt;- res$z[!is.na(res$z)] Figure 5.3: Systematic random sample with orientation N60E 5.1 Estimation of mean and its sampling variance With systematic random sampling all units have an equal inclusion probability. Consequently, the unweighted sample average is a design-unbiased estimator of the mean: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{SY}}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i} \\;. \\tag{5.1} \\end{equation}\\] An unbiased estimator of the sampling variance of the estimated mean is not available. A simple, often applied procedure is to calculate the sampling variance as if the sample were a simple random sample (Equation #eq:HTMeanSI). In general this procedure over-estimates the sampling variance, so that we are on the safe side. Alternatively, the sampling variance can be estimated by treating the SY sample as if it were a stratified simple random sample (Equation #eq:HTMeanSTSI). The sampling units are clustered on the basis of their spatial coordinates into \\(H=n/2\\) clusters (\\(n\\) even) or \\(H=(n-1)/2\\) clusters (\\(n\\) odd). The two (or three) sampling units of a cluster are then treated as a simple random sample from an undefined stratum, and the variance estimator for stratified random sampling is used. With \\(n\\) even the stratum weights are \\(1/H\\) for all strata, with \\(n\\) odd the weights are computed as \\(w_h=n_h/n\\). See Brus and Saby (2016) for a comparison of various variance approximations for SY. Figure 5.4: Clustering of grid points for variance approximation Figure 5.5 shows the sampling distributions of the estimated mean for systematic random sampling with orientation N60E and an expected sample size of 40, and simple random sampling, obtained by repeating the random sampling with each design and estimation 1000 times. For each systematic random sample the sampling variance of the estimated mean is approximated by the two estimators described above. Figure 5.5: Sampling distribution of estimated mean with SY and SI The boxplots of the estimated means indicate that SY is more precise than SI. The variance of the 10^{4} estimated means equals 0.283, whereas for SI this is 0.601. The average of the 10^{4} SI-approximated variances equals 0.642, and of the STSI-approximated variances 0.588. So the STSI approximation is slightly better, but in this case still strongly overestimates the sampling variance. The variation in sample size is much smaller compared to an NS-EW oriented grid, compare Figures 5.2 and 5.6. Figure 5.6: Sampling distribution of sample size of SY with orientation N60E ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 32.00 38.00 40.00 39.97 41.00 46.00 Exercise (SY.R) Write an R script to select a systematic random sample (square grid) with an expected size of 40 from Voorst (data are in Voorst.RData). Overlay the selected points with grdVoorst using function %over% and extract the z-values at the selected SY sample. Estimate the population mean, and approximate its standard error by treating the SY sample as a simple random sample. Cluster the gridpoints using function stratify of package spcosa, and approximate the standard error by treating the sample as a stratified random sample. "],
["pps.html", "Chapter 6 Sampling with probabilities proportional to size 6.1 Probability-proportional-to-size sampling with replacement (ppswr) 6.2 Probability-proportional-to-size sampling without replacement (ppswor) 6.3 Spatial version of ppswor", " Chapter 6 Sampling with probabilities proportional to size In simple random sampling the inclusion probabilities are equal for all sampling units. The advantage of this is simple and straightforward statistical inference. With equal inclusion probabilities the unweighted sample average is an unbiased estimator of the spatial mean, i.e. the sampling design is self-weighting. However, in some situations equal probability sampling will not be very efficient, i.e. given the sample size the precision of the estimated mean or total will be relatively low. An example is the following. In order to estimate the total area of a given crop in a country, a raster of square cells of, for instance, 10 km x 10 km is constructed and projected on the country. The square cells serve as the sampling units. Note that near the country border cells cross the border. Some of them may contain only a few hectares of the target population, the country under study. We do not want to select many of these squares with only a few hectares of the study area, as intuitively it is clear that this will result in low precision of the estimated crop area. In such situation it can be more efficient to select sampling units with probabilities proportional to the area of the target population within the squares, so that small sampling units near the border have smaller probability of being selected than interior sampling units. Actually, the sampling units are not the square cells, but the pieces of land obtained by overlaying the cells and the GIS file of the country under study. This entails that the sampling units have unequal size. These sampling units of unequal size are selected by probabilities proportional to their size. If we have a GIS file of land use categories such as agriculture, build-up areas, water bodies, forests et cetera, we may use this file to further adapt the selection probabilities. The crop will be grown in agricultural areas only, so in cells largely covered by non-agricultural land, we expect small crop areas only. As a size measure in computing the selection probabilities we may use the agricultural area (as represented in the GIS file) in the country under study within the cells. Note that size now has a different meaning. It does not refer to the area of the sampling units anymore, but to an ancillary variable that is hopefully related to the target variable, i.e. the crop area. When the crop area per cell is proportional to the agricultural area per cell, then it can be shown that the precision of the estimated total area of the crop can be increased by selecting the cells with probabilities proportional to the agricultural area. In this example the sampling units have an area. However, sampling with probabilities proportional to size is not restricted to areal sampling units, but can also be used for selecting points. If we have a map depicting an ancillary variable that is expected to be related to the target variable, this ancillary variable can be used as a size measure. For instance, in areas where soil organic carbon is related to (relative) elevation, it can be efficient to select sampling locations proportional to this environmental variable. The ancillary variable must be strictly positive for all locations. Sampling units can be selected with probabilities proportional to their size with or without replacement. This distinction is immaterial for infinite populations. Probability-proportional-to-size sampling is illustrated with a simulated field of poppy area per 5 km \\(\\times\\) 5 km square in the province of Kandahar, Afghanistan3. The agricultural area within the squares is used as a size variable. The scatter plot shows that there is a positive relation between the simulated poppy area and the agricultural area within the sampling units. Clearly the residual variance is not constant, but increases with the agricultural area. Figure 6.1: Scatter plot of poppy area against the size variable agricultural area 6.1 Probability-proportional-to-size sampling with replacement (ppswr) In the first draw a sampling unit is selected with probability \\(p_i = x_i/t(x)\\), with \\(x_i\\) the size variable for sampling unit \\(i\\) and \\(t(x) = \\sum_{i=1}^N x_i\\) the population total of the size variable. The selected unit is then replaced, and these two steps are repeated \\(d\\) times. Note that with this sampling design sampling units can be selected more than once, especially with large sampling fractions \\(d/N\\). For infinite populations this probability density is 0. With this design the spatial mean can be estimated design-unbiased by \\[\\begin{equation} \\hat{\\bar{z}}_{\\mathrm{pps}}=\\frac{1}{Nd}\\sum_{i=1}^{d}\\frac{z_{i}}{p_{i}} \\;, \\tag{6.1} \\end{equation}\\] where \\(d\\) is the number of draws. This estimator is referred to as the Hansen-Hurwitz estimator. The sampling variance can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\mathrm{pps}}\\right)= \\frac{1}{N^2\\,d\\,(d-1)}\\sum_{i=1}^{d}\\left( \\frac{z_{i}}{p_{i}}-N\\,\\hat{\\bar{z}}_{\\mathrm{pps}}\\right)^{2} \\;. \\tag{6.2} \\end{equation}\\] A ppswr sample of 50 draws is selected. The population total and mean of the poppy area is estimated, as well as their standard errors. As a first step I check whether the size variable is strictly positive (&gt;0). The minimum equals 3.066310^{-5}, so this is the case. If this woild not be the case, values equal to or smaller than zero must be replaced by a small number, so that these units also have a positive probability of being selected. #compute draw-by-draw selection probabilities grdKandahar$prob&lt;-grdKandahar$agri/sum(grdKandahar$agri) #set number of draws d&lt;-40 #set seed set.seed(314) N &lt;- nrow(grdKandahar) units&lt;-sample.int(N,size=d,replace=TRUE,prob=grdKandahar$prob) mysample &lt;- grdKandahar[units,] #Hansen-Hurwitz estimator of population total and mean zexpand&lt;-mysample$poppy/mysample$prob totalz&lt;-mean(zexpand) meanz&lt;- totalz/N #standard error of estimated population total and mean setotal&lt;-sqrt(var(zexpand)/d) semean&lt;-setotal/N The estimated total equals 4.2295810^{4} ha, with a standard error of 7190. Two units are selected twice: table(units) ## units ## 23 35 37 38 51 104 106 196 201 225 233 237 281 284 286 293 335 338 ## 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 339 340 354 359 378 382 390 391 407 424 440 447 449 477 488 528 535 564 ## 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 896 898 ## 1 1 Figure 6.2 shows the selected sampling units, plotted on a map of the agricultural area within the units, used as a size variable. Figure 6.2: ppswr sample of 40 draws from Kandahar, using agricultural area as size variable Exercise (pps.R) Write an R script to select a pps sample with replacement from the cotton research farm in Uzbekistan, using EM as a size variable (data are in CottonFarmUzbekistan.RData). Draw 40 times a unit. Estimate the mean ECe with the Hansen-Hurwitz estimator and its sampling variance Challenge: compute the true sampling variances of the the Hansen-Hurwitz estimator and of the Horvitz-Thompson estimator for simple random sampling with replacement and the same sample size. Is there a gain in precision? 6.2 Probability-proportional-to-size sampling without replacement (ppswor) In pps sampling with replacement a sampling unit can be selected more than once, especially with large sampling fractions \\(n/N\\). This may decrease the sampling efficiency. Therefore, pps-sampling without replacement (ppswor) can be an attractive alternative. The selection of sampling units with inclusion probabilities proportional to size without replacement is not straightforward. Various ingenious sampling algorithms have been developed for this, such as Brewer’s method, Sampford’s method and the pivotal method (Deville and Tillé {1998}), see Tillé (2006) for an overview. In ppswor the inclusion probabilities are equal to the draw-by-draw selection probabilities \\(p_i\\) of ppswr, multiplied by the sample size \\(n\\): \\(\\pi_i= n\\;p_i = n\\;x_i/t(x)\\). In the pivotal method the \\(N\\)-vector with inclusion probabilities are successively updated to a vector with indicators. If the indicator for sampling unit \\(i\\) becomes value 1 then this sampling unit is selected, if it becomes 0 then it is not selected. One step in the updating algorithm can be described as follows: select fully randomly two sampling units \\(k\\) and \\(l\\) with \\(0&lt;\\pi_k&lt;1\\) and \\(0&lt;\\pi_l&lt;1\\) If \\(\\pi_k + \\pi_l &lt; 1\\) then update the probabilities by \\[\\begin{equation} (\\pi^{\\prime}_k,\\pi^{\\prime}_l)=\\left\\{ \\begin{array}{cc} (0,\\pi_k+\\pi_l) &amp; \\;\\;\\;\\text{with probability}\\frac{\\pi_l}{\\pi_k+\\pi_l} \\\\ (\\pi_k+\\pi_l,0) &amp; \\;\\;\\;\\text{with probability}\\frac{\\pi_k}{\\pi_k+\\pi_l} \\end{array} \\right. \\tag{6.3} \\end{equation}\\] and if \\(\\pi_k + \\pi_l \\geq 1\\) then update the probabilities by \\[\\begin{equation} (\\pi^{\\prime}_k,\\pi^{\\prime}_l)=\\left\\{ \\begin{array}{cc} (1,\\pi_k+\\pi_l-1) &amp; \\;\\;\\;\\text{with probability}\\frac{1-\\pi_l}{2-(\\pi_k+\\pi_l)} \\\\ (\\pi_k+\\pi_l-1,1) &amp; \\;\\;\\;\\text{with probability}\\frac{1-\\pi_k}{2-(\\pi_k+\\pi_l)} \\end{array} \\right. \\tag{6.4} \\end{equation}\\] Replace (\\(\\pi_k,\\pi_l\\)) by (\\(\\pi^{\\prime}_k,\\pi^{\\prime}_l\\)), and repeat the first two steps until all units are either selected or not selected. In words, when the sum of the inclusion probabilities is smaller than 1, the updated inclusion probability of one of the points will become 0, which means that this point will not be sampled. The inclusion probability of the other point will become the sum of the two inclusion probabilities, which means that the probability that this point will be selected in one of the subsequent iterations increases. The probability of a point of being excluded from the sample is proportional to the inclusion probability of the other point, so that the larger the inclusion probability of the other point, the larger the probability that it will not be selected. When the sum of the inclusion probabilities of the two points is larger than or equal to 1, then one of the points is selected (updated inclusion probability is 1), while the inclusion probability of the other is lowered by 1 minus the inclusion probability of the selected point. The probability of being selected is proportional to the complement of the inclusion probability of the other point. With this ppswor design the total can be estimated by the Horvitz-Thompson estimator: \\[\\begin{equation} \\hat{t}_{\\text{ppswor}}(z)=\\sum_{i=1}^{n}\\frac{z_{i}}{\\pi_{i}} \\;, \\tag{6.5} \\end{equation}\\] where \\(n\\) is the number of selected sampling units. Several approximate variance estimators have been developed (Matei and Tillé 2005). With small sampling fractions the Hansen-Hurwitz variance estimator can be used. In the sampling experiment on poppy area in Kandahar (see R chunk hereafter) this approximation overestimates the sampling variance. With large sampling fractions the variance can be approximated by \\[\\begin{equation} \\widehat{V}(\\hat{t}_{\\text{ppswor}}(z))=\\frac{1}{1-\\sum_{i\\in s} a_i^2}\\sum_{i\\in s}(1-\\pi_i)\\left(\\breve{z}_i-A\\right)^2 \\;, \\tag{6.6} \\end{equation}\\] where \\(\\breve{z}_i=z_i/\\pi_i\\), \\(a_i=(1-\\pi_i)/\\sum_{j\\in s} (1-\\pi_j)\\), and \\(A=\\sum_{i\\in s} a \\breve{z}_i\\) ppswor samples can be selected with varuious functions in R package sampling. I used the function UPpivotal. library(sampling) #set sample size n &lt;- 40 eps &lt;- 1e-3 set.seed(314) ppswor &lt;- function(sframe, size, n) { pik &lt;- inclusionprobabilities(size,n) sampleind&lt;-UPpivotal(pik=pik,eps=eps) mysample &lt;- data.frame(sframe[sampleind&gt;eps,],pik=pik[sampleind&gt;eps]) return(mysample) } mysample &lt;- ppswor(sframe=grdKandahar,size=ifelse(grdKandahar$agri&lt;1E-12,0.1,grdKandahar$agri), n=n) totppsHT&lt;-sum(mysample$poppy/mysample$pik) pk &lt;- mysample$pik/n #approximate variance by variance of Hansen-Hurwitz estimator vartotHH&lt;-var(mysample$poppy/pk)/n Figure 6.3 shows the sampling distributions of the estimated total poppy area with ppswor sampling and simple random sampling without replacement of size 40, obtained by repeating the random sampling with each design and estimation 1000 times. For each ppswor sample the variance of the Horvitz-Thompson estimator is approximated by the variance of the Hansen-Hurwitz estimator. Figure 6.3: Sampling distribution of estimated total poppy area with ppswor and SI Sampling design ppswor is clearly much more precise than simple random sampling. The standard deviation of the Horvitz-Thompson estimator of the total poppy area with ppswor equals 5757. The average of the square root of the approximated variances equals 6218. Questions A field with poppy was found outside Kandahar in a selected sampling unit crossing the boundary. Should this field be included in the sum of the poppy area of that sampling unit? In another sampling unit a poppy field was encountered in Kandahar but in the area represented as non-agriculture in the GIS file. Should this field be included in the sum of that sampling unit? 6.3 Spatial version of ppswor The units selected with ppswr and ppswor may show strong spatial clustering, which inflates the standard error of the estimated population total and mean. To avoid spatial clustering Grafström, Lundström, and Schelin (2012) developed the so-called local pivotal method for selecting a ppswor sample. The only diffrence with the pivotal method described above is the selection of the pairs of points. In the pivotal method the two points of a pair are selected independently, whereas in the local pivotal method the first point is selected fully randomly and the nearest neighbour of this point is used as its counterpart. Recall that when one point of a pair is included in the sample, the inclusion probability of its counterpart is decreased. This leads to a better spreading of the sampling units in geographic space. Selecting samples with the local pivotal method can be done with function lpm of R package BalancedSampling. Figure 6.4 shows the sample of 40 units selected from the sampling frame of Kandahar. pik &lt;- inclusionprobabilities(grdKandahar$agri,n) X&lt;- cbind(grdKandahar$x,grdKandahar$y) units &lt;- lpm(pik,X,h=N) mysample &lt;- grdKandahar[units,] Figure 6.4: Spatial ppswor sample selected by local pivotal method, using agricultural area as size variable The field is simulated with a model calibrated on sample data of poppy area. The field was simulated unconditionally on these sample data. The simulated field may therefore differ substantially from the real world↩ "],
["Cl.html", "Chapter 7 Cluster random sampling 7.1 Estimation of mean and its sampling variance 7.2 Stratified cluster random sampling", " Chapter 7 Cluster random sampling With stratified random sampling with geographical strata and systematic random sampling the sampling units are well spread throughout the study area. In general this leads to an increase of the precision of the estimated mean (total). With large study areas the price to be paid for this is long travel times, so that less sampling units can be observed in a given survey period. In this situation it can be more efficient to select spatial clusters of sampling units. In cluster random sampling, once a cluster is selected, all sampling units in this cluster are observed. For this reason this design is also referred to as single-stage cluster random sampling. The clusters are not subsampled, as in two-stage random sampling (see Chapter 8). A popular cluster shape is a transect. The reason is that the individual sampling units of a transect can easily be located in the field, which was a big advantage in the pre-GPS era. The implementation of cluster random sampling is not straightforward. I have seen many examples in the literature of an improper implementation of this sampling design. A proper selection technique is as follows (de Gruijter et al. 2006). In the first step a `starting point (unit)’ is selected, for instance by simple random sampling. Then the remaining units of the cluster to which the starting point belongs are identified by making use of the definition of the cluster. For instance, with clusters defined as E-W oriented transects, with a cluster spacing of 100 m, all points east and west of the starting point at a distance of 100 m, 200 m et cetera that fall inside the study area are selected. These two steps are repeated until the required number of clusters is selected. A requirement of a valid selection method is that the same cluster is selected, regardless of which of its locations is used as a starting point. In the example above this is the case: regardless of which of the points on the transect is selected first, the final set of points selected is the same. An example of an improper implementation of this sampling design is the following. A cluster is defined as an E-W oriented transect of four points with a mutual spacing of 100 m. A cluster is selected by randomly selecting a starting point. The remaining three points of the cluster are selected E of this starting point. Points outside the study area are ignored. With this selection method the set of selected points is not independent of the starting point, and therefore is invalid. Note that the size of the clusters, i.e. the number of points (units) of a cluster need not be constant. With the proper selection method described above the selection probability of a cluster is proportional to its size (pps-sampling). With irregularly shaped study areas the size of the cluster can vary strongly. The size of the clusters can be controlled by subdividing the study area into blocks, for instance stripes perpendicular to the direction of the transects, or square blocks in case the clusters are grids. In this case, the remaining units are identified by extending the transect or grid until the boundary of the block. With irregularly shaped areas blocking will not eliminate entirely the variation in cluster sizes. The R code below shows the selection of E-W oriented transects in Voorst. In order to delimit the length of the transects the study area is split into six 1 km \\(\\times\\) 1 km blocks. Note that these blocks do not serve as strata. When used as strata, from each block one or more clusters would be selected, see Section 7.2. As a first step in the R script all clusters in the finite representation of the population are constructed. The cluster-id is added to the sampling frame. Each point belongs exactly to one cluster. Then several clusters are selected with probabilities proportional to size and with replacement (ppswr). This is done by simple random sampling with replacement of points, and identifying the clusters to which these points belong. Finally, all points of the selected clusters are included in the sample. #compute local coordinates s1local &lt;- grdVoorst$s1-min(grdVoorst$s1) s2local &lt;- grdVoorst$s2-min(grdVoorst$s2) #set spacing of points within clusters spacing &lt;- 100 s1f &lt;- as.factor(s1local%%spacing) s2f &lt;- as.factor(s2local) #construct clusters (E-W oriented transects within zones) grdVoorst$cluster &lt;- as.character(interaction(s1f,s2f,as.factor(grdVoorst$zone))) grdVoorst$id &lt;- seq(1:nrow(grdVoorst)) #now define function for cluster random sampling cl &lt;- function(sframe,d) { units&lt;-sample.int(nrow(sframe),size=d,replace=TRUE) ids.pnt&lt;-sframe$id[units] ids.cl &lt;- sframe$cluster[units] mysample &lt;- sframe[sframe$cluster %in% ids.cl,] mysample$start &lt;- 0 mysample$start[mysample$id%in%ids.pnt] &lt;- 1 return(mysample) } #set number of cluster draws d&lt;-4 set.seed(314) mysample &lt;- cl(sframe=grdVoorst,d=d) cls &lt;- unique(mysample$cluster) cellsize&lt;-25 for (i in 1:d) { mysample$s1[mysample$cluster==cls[i]] &lt;- mysample$s1[mysample$cluster==cls[i]]+runif(1,min=-cellsize/2,max=cellsize/2) mysample$s2[mysample$cluster==cls[i]] &lt;- mysample$s2[mysample$cluster==cls[i]]+runif(1,min=-cellsize/2,max=cellsize/2) } Figure 7.1 shows the selected sample. The total number of selected points equals 36. Similar to systematic random sampling, with cluster random sampling the total sample size is random, so that we do not have perfect control of the total sample size. This is because in this case the sizes (number of points) of the clusters is not constant but varies. Figure 7.1: Cluster random sample from Voorst The output mysample of function cl in the chunk above has a column named `start’. This is an indicator with value 1 if this point of the cluster is selected first, and 0 else. When in the field it appears that the first selected point of a cluster does not belong to the target population, all other points of that cluster are also dropped. This is to keep the selection probabilities of the clusters exactly proportional to their size. 7.1 Estimation of mean and its sampling variance With pps-sampling with replacement of clusters the population mean is estimated unbiasedly by (see chunk hereafter): \\[\\begin{equation} \\hat{\\bar{z}}_{\\mathrm{Cl}}= \\frac{1}{d}\\sum\\limits_{i=1}^{d}\\hat{\\bar{z}}_{i} \\;, \\tag{7.1} \\end{equation}\\] where \\(d\\) is the number of times a cluster is drawn (number of draws) and \\(\\hat{\\bar{z}}_{i}\\) is the sample mean of cluster \\(i\\). The sampling variance can be estimated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\mathrm{Cl}}\\right)=\\frac{\\widehat{S^2}_{\\mathrm{Cl}}}{d} \\;, \\tag{7.2} \\end{equation}\\] where \\(\\widehat{S^2}_{\\mathrm{Cl}}\\) is the estimated variance of estimated cluster means (between cluster variance) \\[\\begin{equation} \\widehat{S^2}_{\\mathrm{Cl}} = \\frac{1}{d-1}\\sum_{i=1}^{d}(\\hat{\\bar{z}}_{i}-\\hat{\\bar{z}}_{\\mathrm{Cl}})^2 \\;, \\tag{7.3} \\end{equation}\\] clusterMeans&lt;-tapply(mysample$z,INDEX=mysample$cluster,FUN=mean) estimatedMean&lt;-mean(clusterMeans) estimatedVarMean&lt;-var(clusterMeans)/d Note that the size of the clusters (number of locations) does not appear in these formulas. This simplicity is due to the fact that the clusters are selected with probabilities proportional to size. The effect of the cluster size on the variance is implicitly accounted for. To understand this, consider that larger clusters result in smaller variance among their means. Figure 7.2 shows the sampling distributions of the estimated mean with cluster random sampling and simple random sampling, obtained by repeating the random sampling with each design and estimation 10,000 times. For simple random samples the size is equal to the expected sample size of the cluster random sampling design. Figure 7.2: Sampling distribution of estimated mean with Cl and SI The variance of the 10^{4} means with cluster random sampling equals 1.375. This is considerably larger than with simple random sampling: 0.725. The large variance is cause by the strong spatial clustering of points. This may save travel time, although in this small study area this is possibly rather limited. For large study areas the saving of travel time in the field with cluster random sampling can be considerable. The average of the estimated variances with cluster random sampling equals 1.326. This indicates that the estimator of the variance, Equation (7.2), is unbiased. Figure 7.3 shows the sampling distribution of the sample size. The expected sample size equals 33. Figure 7.3: Sampling distribution of sample size 7.2 Stratified cluster random sampling The basic sampling designs stratified random sampling (Chapter 4) and cluster random sampling can be combined into stratified cluster random sampling. So instead of selecting simple random samples from the strata, clusters are randomly selected. Figure 7.4 shows a stratified cluster random sample from Voorst. The strata consist of three 2 km \\(times\\) 1 km blocks, obtained by joining two neighbouring 1 km \\(times\\) 1 km blocks (Figure 7.1). The clusters are the same as before, i.e. E-W oriented transects within 1 km \\(times\\) 1 km blocks, with a inter-point spacing of 100 m. Within each stratum two times a cluster is selected by ppswr. The stratification avoids the clustering of the selected transects in one part of the study area. Compared to (unstratified) cluster random sampling, the geographical spreading of the clusters is improved, which may lead to an increase of the precision of the estimated population mean. Figure 7.4: Stratified cluster random sample from Voorst The population mean is estimated by first estimating the stratum means using Equation (7.1), followed by computing the weighted average of the estimated stratum means using Equation (4.2). The variance of the estimated population mean is estimated in the same way, by first estimating the variance of the estimated stratum means using Equation (7.2), followed by computing the weighted average of the estimated variances of the estimated stratum means (Equation (4.3)). Exercise (StratifiedCluster.R) Write an R script to select a stratified cluster random sample from Voorst, using the 2 km \\(\\times\\) 1 km blocks of Figure 7.4 as strata. Select in each stratum two times a cluster with probabilities proportional to size Estimate the population mean and its standard error Questions Why is it attractive to select at least two clusters per stratum? "],
["Twostage.html", "Chapter 8 Two-stage random sampling 8.1 Estimation of mean and its sampling variance 8.2 Stratified two-stage random sampling", " Chapter 8 Two-stage random sampling This sampling design is also referred to as two-stage cluster random sampling (Lohr 1999). The difference with (single-stage) cluster random sampling (Chapter 7) is that not all sampling units of the selected clusters are observed, but only some of them. Also, here the clusters will generally be contiguous groups of sampling units, for instance all locations in a single polygon, whereas in single-stage cluster random sampling the clusters generally are non-contiguous. The sampling units to be observed are selected by random subsampling of the randomly selected clusters. In two-stage sampling the clusters are commonly referred to as primary sampling units (psu’s) or shortly primary units (pu’s), and the units selected in the second stage as the secondary sampling units (ssu’s) or secondary units (su’s). As with cluster random sampling, two-stage random sampling may lead to a strong spatial clustering of the selected sampling units (ssu’s) in the study area. This may save considerable time for fieldwork, and more sampling units can be observed for the same budget. However, due to the spatial clustering the precision of the estimates will generally be less precise compared to samples of the same size selected by a design that leads to a much better spreading of the sampling units throughout the study area, such as systematic random sampling. In two-stage random sampling in principle any type of sampling design can be used at the two stages, leading to numerous combinations. A simple version is (SI,SI), in which both psu’s and ssu’s are selected by simple random sampling. Commonly the psu’s have unequal size, i.e., the number of ssu’s (finite population) or the area (infinite population) are not equal for all psu’s. Think for instance of the agricultural fields, forest stands, lakes, river sections et cetera in an area. If the psu’s are of unequal size, then psu’s can best be selected with probabilities proportional to their size (pps). If the total of a psu is related to its size, then pps-sampling leads to more precise estimates compared to simple ranmdom sampling of psu’s. Also, with pps-sampling of psu’s the estimation of means or totals and of their sampling variances is much simpler compared to selection with equal probabilities. Implementation of selection with probabilities proportional to size is easiest `when units are replaced (pps with replacement, ppswr). This implies that a psu might be selected more than once, especially if the total number of psu’s in the population is small compared to the number of psu draws (large sampling fraction in first stage). The following algorithm can be used to select \\(d\\) times a psu by ppswr from a total of \\(N\\) psu’s in the population: 1. Select randomly one ssu from the list with \\(M=\\sum_{i=1}^N M_i\\) ssu’s (\\(M_i\\) is number of ssu’s of primary unit \\(i\\)), and determine the psu of the selected ssu 2. Repeat step 1 until \\(d\\) selections have been made. This sampling algorithm makes use of a list of all sampling units as a sampling frame. An alternative algorithm works with a geographical representation of the study area (shape file) and its psu’s: Select a random point in the area, as in SI. Use a point-in-polygon routine to determine in which psu the point falls, and select this psu. Repeat steps 1 and 2 until \\(d\\) selections have been made. In the second stage, a pre-determined number of secondary sampling units, \\(m_{i}\\), is selected every time psu \\(i\\) is selected. Secondary sample sizes Predetermined means that it is not allowed to decide on the secondary sample sizes (number of ssu’s) per selected psu after the selection of the psu’s. The R code below shows the selection of a two-stage random sample from Voorst. Primary units are 24 blocks of 0.5 km \\(\\times\\) 0.5 km. Note that these blocks have unequal size, i.e. the number of pixels within the blocks differs due to build-up area, roads et cetera. Four times a psu is selected, with probabilities proportional to size and with replacement. The second stage sample size \\(m_i\\) equals 10 for all psu’s. These units ssu’s are selected by simple random sampling. d &lt;- 4 #number of psu selections m &lt;- 10 #fixed number of ssu selections per psu-draw #define function for two-stage random sampling twostage &lt;- function(sframe,psu,d,m) { units&lt;-sample.int(nrow(sframe),size=d,replace=TRUE) mypsusample&lt;-sframe[units,psu] i&lt;-1 ssunits &lt;- NULL for (psunit in mypsusample) { ssunit &lt;- sample(x = which(sframe[,psu] == psunit), size = m, replace=TRUE) ssunits &lt;- c(ssunits, ssunit) i&lt;-i+1 } mysample &lt;- sframe[ssunits,] return(mysample) } set.seed(314) mysample &lt;- twostage(sframe=grdVoorst,psu=&quot;psu&quot;,d=d,m=m) cellsize &lt;- 25 mysample$s1 &lt;- jitter(mysample$s1,cellsize/2) mysample$s2 &lt;- jitter(mysample$s2,cellsize/2) Figure 8.1 shows the selected sample. Figure 8.1: Two-stage random sample from Voorst 8.1 Estimation of mean and its sampling variance If the psu’s are selected with replacement and with probabilities proportional to size (ppswr), then the population mean can be estimated by the simple estimator (see chunk hereafter): \\[\\begin{equation} \\hat{\\bar{z}}_{\\mathrm{(pps,SI)}}= \\frac{1}{d}\\sum\\limits_{i=1}^{d}\\hat{\\bar{z}}_{i} \\;, \\tag{8.1} \\end{equation}\\] where \\(d\\) is the number of psu selections, and \\(\\hat{\\bar{z}}_{i}\\) is the sample mean of the psu selected in the \\(i^{th}\\) draw. The strategy ((pps,SI), \\(\\hat{\\bar{z}}_{\\mathrm{(pps,SI)}}\\)) is design-unbiased. The sampling variance of the estimated mean can simply be estimated by: \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\mathrm{(pps,SI)}}\\right)=\\frac{\\widehat{S^2}_{\\mathrm{psu}}}{d} \\;. \\tag{8.2} \\end{equation}\\] with \\(\\widehat{S^2}_{\\mathrm{psu}}\\) the estimated variance of estimated primary unit means (between primary unit variance) \\[\\begin{equation} \\widehat{S^2}_{\\mathrm{psu}} = \\frac{1}{d-1}\\sum_{i=1}^{d}(\\hat{\\bar{z}}_{i}-\\hat{\\bar{z}}_{\\mathrm{(pps,SI)}})^2 \\;, \\tag{8.3} \\end{equation}\\] Note that neither the sizes of the psu’s, i.e. the number of ssu’s per primary unit, nor the secondary sample sizes \\(m_{i}\\) occur in these formulas. This simplicity is due to the fact that the psu’s are selected with replacement and with probabilities proportional to size. The effect of the secondary sample sizes on the variance is implicitly accounted for. To understand this, note that the larger \\(n_{i}\\), the less variable \\(\\hat{\\bar{z}}_{i}\\), and the smaller its contribution to the variance. psumeans &lt;- tapply(mysample$z,INDEX=mysample$psu,FUN=mean) meanz &lt;- mean(psumeans) Vmeanz &lt;- var(psumeans) / d Figure 8.2 shows the sampling distributions of the estimated mean with two-stage random sampling and simple random sampling, obtained by repeating the random sampling with each design and estimation 10,000 times. Figure 8.2: Sampling distribution of estimated mean with Twostage random sampling and SI The variance of the 10^{4} means with two-stage random sampling equals 1.383. This is considerably larger than with SI: 0.592. The average of the estimated variances equals 1.431. Questions Is the standard error of the estimated mean with 10 psu draws and 4 ssu’s per psu draw (\\(10 \\times 4\\)) larger or smaller than with \\(4 \\times 10\\)? 8.2 Stratified two-stage random sampling The basic sampling designs stratified random sampling (Chapter 4) and two-stage random sampling can be combined into stratified two-stage random sampling. Figure 8.3 shows a stratified two-stage random sample from Voorst. The strata are as before in stratified cluster random sampling (Figure 7.1). The primary sampling units are as before in (unstratified) two-stage random sampling (Figure 8.1). Within each stratum two times a psu is selected by ppswr, and every time a psu is selected, six ssu’s (points) are selected by simple random sampling. The stratification avoids the clustering of the selected psu’s in one part of the study area. Compared to (unstratified) two-stage random sampling, the geographical spreading of the psu’s is somewhat improved, which may lead to an increase of the precision of the estimated population mean. Figure 8.3: Stratified twostage random sample from Voorst The population mean is estimated by first estimating the stratum means using Equation (8.1), followed by computing the weighted average of the estimated stratum means using Equation (4.2). The variance of the estimated population mean is estimated in the same way, by first estimating the variance of the estimated stratum means using Equation (8.2), followed by computing the weighted average of the estimated variances of the estimated stratum means (Equation (4.3)). Exercise (StratifiedTwoStage.R) Write an R script to select a stratified two-stage random sample from Voorst, using the 2 km \\(\\times\\) 1 km blocks of Figure 8.3 as strata and the 1 km \\(\\times\\) 1 km blocks as primary sampling units (psu’s). Select in each stratum two times a psu by ppswr, and select six ssu’s per psu draw. Estimate the population mean and its standard error "],
["RegressionEstimator.html", "Chapter 9 Regression and ratio estimator 9.1 Regression estimator 9.2 Ratio estimator 9.3 Regression estimator with unknown mean of covariate", " Chapter 9 Regression and ratio estimator This Chapter is about the use of covariates in design-based estimation from probability samples. In many cases ancillary information is available that could be useful to increase the precision of the estimated mean or total of the study variable. The ancillary variable(s) can be qualitative (i.e., classifications) or quantitative. As we have seen before both types of ancillary variable can be used at the selection stage (design stage) to improve the performance of the sampling strategy, for instance by stratification (Chapter 4) or selecting sampling units with probabilities proportional to size (Chapter 6). In this Chapet I explain how these covariates can be used at the stage of estimation. 9.1 Regression estimator Suppose we have observed along with the study variable a quantitative variable that is linearly related to the study variable. Further suppose that we know the population mean of this covariate. Then for simple random sampling the mean of the study variable can be estimated by the regression estimator: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{regr}}= \\bar{z}_s+b\\left( \\bar{x}-\\bar{x}_s\\right) \\;, \\tag{9.1} \\end{equation}\\] where \\(\\bar{z}_s\\) is the sample mean of the study variable, \\(\\bar{x}_s\\) is the sample mean of the covariate, \\(\\bar{x}\\) is the population mean of the covariate, and \\(b\\) is the estimated regression coefficient (estimated slope). Note that for this estimator the population mean of the covariate must be known. So, if we know the covariate values only at the sampling points, then this estimator cannot be used. The regression coefficient \\(b\\) is estimated by the least squares estimator: \\[\\begin{equation} b=\\frac{\\sum_{i=1}^n(x_i-\\bar{x}_s)(z_i-\\bar{z}_s)}{\\sum_{i=1}^n(x_j-\\bar{x}_s)^2} \\tag{9.2} \\end{equation}\\] The rationale of the regression estimator is that when the estimated mean of the covariate is, for instance, smaller than the population mean of the covariate, then with a positive correlation between the study variable and covariate, also the estimated mean of the study variable is expected to be smaller than the population mean of the study variable. The difference between the population mean and estimated mean of the covariate can be used to improve the simple random sample estimate of the mean of \\(z\\), by adding a term proportional to the difference between the estimated mean and population mean of the covariate. As a proportion the slope of the regression line is used. The sampling variance of this regression estimator can be estimated by computing first the regression residuals \\(e_i= z_i - \\hat{z}_i, i = 1 \\cdots n\\) at the sampling points. The sampling variance of the regression estimator is approximately equal to the sampling variance of the estimated mean of these residuals: \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\mathrm{regr}}\\right)=\\frac{\\widehat{S^{2}}(e)}{n} \\;, \\tag{9.3} \\end{equation}\\] with \\(\\widehat{S^{2}}(e)\\) the estimated population variance of the regression residuals \\[\\begin{equation} \\widehat{S^{2}}(e)=\\frac{1}{(n-2)}\\sum\\limits_{i=1}^{n}\\left(e_{i}\\right)^{2} \\tag{9.4} \\end{equation}\\] The variance estimator is an approximation because the regression coefficient is also estimated from the sample, which makes the regression estimator nonlinear. For simple random sampling without replacement from finite populations, the variance estimator must be multiplied by \\(1-n/N\\). The regression estimator is illustrated with the electrical conductivity (ECe) data at the cotton farm in Uzbekistan, see Figure 4.4. As a covariate I use the electromagnetic measurements of the apparent electrical conductivity (EM). Figure 9.1 shows a scatter plot for all grid nodes (referred to as the exhaustive scatter plot). Figure 9.1: Exhaustive scatter plot of ECe against EM The correlation coefficient equals 0.593. The slope of the fitted line equals 0.041. Now a simple random sample of 40 points is selected, and a simple linear model is fitted. n&lt;-40 set.seed(314) ids &lt;- sample.int(nrow(grdUzbekistan), size = n, replace = FALSE) mysample&lt;-grdUzbekistan[ids,c(&quot;ECe&quot;,&quot;EM&quot;)] #fit simple linear model lmsample&lt;-lm(ECe~EM,data=mysample) ab&lt;-coef(lmsample) Figure 9.2 shows the scatter plot for the sample and the fitted line. Figure 9.2: Scatterplot and fitted line for simple random sample The simple random sample is used to estimate the population mean of the study variable ECe by the regression estimator, and to approximate the sampling variance of the regression estimator. populationmeanx&lt;-mean(grdUzbekistan$EM) #population mean of covariate samplemeanx&lt;-mean(mysample$EM) #sample mean of covariate samplemeanz&lt;-mean(mysample$ECe) #sample mean of study variable regressionestimate&lt;-samplemeanz+ab[2]*(populationmeanx-samplemeanx) #regression estimate e&lt;-residuals(lmsample) S2e&lt;-sum(e^2)/(n-2) #estimated variance of residuals varregressionestimate&lt;-S2e/n #estimated approximate variance of regression estimator The difference \\(\\delta(x)\\) between the population mean of the covariate EM (49.947) and its estimated mean (41.984) equals 7.963. We may expect the difference between the unknown population mean of the study variable ECe and its sample mean (2.078) to be equal to \\(\\delta(x)\\), multiplied by the estimated slope of the line, which equals 0.0194. The result (0.1545)is added to the simple random sample estimate. The resulting regression estimate (2.232) is closer to the population spatial mean of \\(z\\) (2.586) than the simple random sample estimate (2.078). The estimated approximate variance of the regression estimator equals 0.021.Note that in approximating the variance of the regression estimator, the variance of the residuals is estimated by dividing the sum of squared residuals by \\(n-2\\), not \\(n-1\\). The reason is that we estimated two regression coefficients, the intercept and slope, so that we loose two degrees of freedom. Figure @(fig:SamplingDistributionRegression) shows the sampling distribution of the simple regression estimator, obtained by repeating the random sampling and estimation 10,000 times. Figure 9.3: Sampling distribution of regression and Horvitz-Thompson estimator for SI of size 40 The average of the 10,000 regression estimates equals 2.591. The population mean of the study variable ECe equals 2.586. so the estimated bias of the regression estimator equals 0.005. The variance of the 10,000 regression estimates equals 0.037, and the average of the 10,000 estimated approximate variances equals 0.035. The gain in precision due to the regression estimator, quantified by the ratio of the variance of the Horvitz-Thompson estimator to the variance of the regression estimator equals 1.508. Exercise (RegressionEstimator.R) Write an R script to select a simple random sample without replacement of size 40 from the cotton farm in Uzbekistan (data are CottonFarmUzbekistan.RData). Estimate the mean electrical conductivity (ECe) by the regression estimator using the electromagnetic measurements (EM) as a covariate Approximate the sampling variance of the regression estimator Repeat the sampling 10,000 times in a for-loop, compute the regression estimator for every sample, and compute the variance of the 10,000 regression estimates Compute the true sampling variance of the Horvitz-Thompson estimator of the same size, and compute the gain in precision achieved by the regression estimator 9.1.1 Regression estimator for stratified simple random sampling With stratified simple random sampling sampling there are two regression estimators, the separate and the combined regression estimator. In the first estimator the regression estimator for simple random sampling is applied at the level of the strata. This implies that for each stratum separately a regression coefficient \\(b_h\\) is estimated. The regression estimates of the stratum means are then combined by computing the weighted average, usng the relative sizes of the strata as weights: \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{sepreg}}=\\sum_{h=1}^H w_h \\hat{\\bar{z}}_{\\text{regr,}h} \\tag{9.5} \\end{equation}\\] with \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{regr,}h}= \\bar{z}_{s,h}+b_h\\left( \\bar{x}_h-\\bar{x}_{s,h}\\right) \\tag{9.6} \\end{equation}\\] with \\(\\bar{z}_{s,h}\\) and \\(\\bar{x}_{s,h}\\) the stratum sample means of the study variable and covariate, respectively, \\(\\bar{x}_h\\) the mean of the covariate in stratum \\(h\\), and \\(b_h\\) the estimated slope coefficient for stratum \\(h\\). The variance of the seperate regression estimator of the population mean can be estimated by first estimating the variances of the regression estimators of the stratum means using Equation (9.3), and then combining these variances using Equation (4.3). The alternative is the combined regression estimator \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{combreg}}= \\hat{\\bar{z}}_{\\text{STSI}}+b_{\\text{STSI}}\\left( \\bar{x}-\\hat{\\bar{x}}_{\\text{STSI}}\\right) \\tag{9.7} \\end{equation}\\] with \\[\\begin{equation} b_{\\text{STSI}} = \\frac{\\sum_{h=1}^H w_h^2 \\widehat{S^2}_h(x,z)/n_h}{\\sum_{h=1}^H w_h^2 \\widehat{S^2}_h(x)/n_h} \\tag{9.8} \\end{equation}\\] with \\(\\widehat{S^2}_h(x,z)\\) the estimated covariance of the study variable \\(z\\) and the covariate \\(x\\) in stratum \\(h\\): \\[\\begin{equation} \\widehat{S}^2_h(x,z)=\\frac{1}{n_h\\,(n_h-1)}\\sum\\limits_{i=1}^{n_h} (z_{hi}-\\bar{z}_{s,h})(x_{hi}-\\bar{x}_{s,h}) \\tag{9.9} \\end{equation}\\] and \\(\\widehat{S^2}_h(x)\\) the estimated variance of the covariate \\(x\\) in stratum \\(h\\) \\[\\begin{equation} \\widehat{S^2}_h(x)=\\frac{1}{n_h\\,(n_h-1)}\\sum\\limits_{i=1}^{n_h} (z_{hi}-\\bar{z}_{s,h})^2 \\tag{9.10} \\end{equation}\\] In the combined regression estimator only one regression coefficient \\(b\\) is estimated, which is an estimate of the population regression coefficient (the slope of the line in the exhaustive scatterplot). This combined regression estimator is recommendable when the stratum sample sizes are small, so that the estimated regression coefficients per stratum, \\(b_h\\), become unreliable. The estimators above are for infinite populations and for stratified simple random sampling with replacement of finite populations. For sampling without replacement from finite populations, finite population corrections \\(1-n_h/N_h\\) must be added to the numerator and denominator of \\(b_{\\text{STSI}}\\), see p. 202 in Cochran (1977). The variance of the combined regression estimator can be estimated as follows: 1. estimate the intercept: \\(a_{\\text{STSI}} = \\hat{\\bar{z}}_{\\text{STSI}}-b_{\\text{STSI}}\\bar{x}\\) 2. compute residuals: \\(e_i = z_i - (a_{\\text{STSI}} + b_{\\text{STSI}} x_i)\\) 3. estimate for each stratum the variance of the estimated mean of the residuals: \\(\\widehat{V}\\!\\left(\\hat{\\bar{e}}_h\\right)=\\widehat{S^{2}}_h(e)/n_h\\), with \\(\\widehat{S^{2}}_h(e)\\) the estimated variance of the residuals in stratum \\(h\\) 4. combine the estimated variances per stratum: \\(\\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{combreg}}\\right)=\\sum_{h=1}^Hw^2_h\\widehat{V}\\!\\left(\\hat{\\bar{e}}_h\\right)\\) Exercise (RegressionEstimator_STSI.R) Write an R script to estimate the mean ECe of the cotton farm in Uzbekistan by stratified simple random sampling. Construct two strata on the basis of covariate EM, using as a threshold 50. Select a stratified simple random sample without replacement of size 40, and allocate the sample to the strata proportional to their size Estimate the mean ECe by the seperate regression estimator and approximate the sampling variance of this regression estimator Estimate the mean ECe by the combined regression estimator and approximate the sampling variance of this regression estimator 9.2 Ratio estimator In some cases it is reasonable to assume that the fitted line goes through the origin. An example is the case study on poppy area in Kandahar (Chapter 6). In this case study the covariate is the agricultural area within the 5 km \\(\\times\\) 5 km squares that serve as sampling units. Assuming that this covariate is determined without error, it is reasonable to assume that when the covariate equals zero, also the poppy area is zero. This implies that when in fitting a straight line through a scatter plot of poppy area against agricultural area within sampling units, this line must pass the origin. In other words a model without intercept should be fitted. Hereafter it is shown that when the line is fitted with weights inversely proportional to the covariate, the slope of the fitted line equals the ratio of the population total poppy area and the population total agricultural area. mod&lt;-lm(poppy~agri-1,data=grdKandahar,weights=1/agri) (slope &lt;- mod$coef) ## agri ## 0.1315106 populationtotalz&lt;-sum(grdKandahar$poppy) populationtotalx&lt;-sum(grdKandahar$agri) (ratio&lt;-populationtotalz/populationtotalx) ## [1] 0.1315106 So, if we have a random sample, the slope of the regression line through the origin can simply be estimated by the ratio of the estimated total of the covariate \\(x\\) to the estimated total of the study variable \\(z\\), which is equal to the ratio of the estimated mean of \\(x\\) to the estimated mean of \\(y\\): \\[\\begin{equation} b=\\frac{\\hat{t}(z)}{\\hat{t}(x)}=\\frac{\\hat{\\bar{z}}}{\\hat{\\bar{x}}} \\tag{9.11} \\end{equation}\\] with \\(\\hat{t}(z)\\) and \\(\\hat{t}(x)\\) the estimated totals of the study variable and ancillary variable, respectively, and \\(\\hat{\\bar{z}}\\) and \\(\\hat{\\bar{x}}\\) the estimated means of the study variable and ancillary variable, respectively. The total of the study variable \\(z\\) can then be estimated by multiplying the total of the covariate \\(x\\) by this ratio: \\[\\begin{equation} \\hat{t}_{\\mathrm{ratio}}(z)=b \\;t(x) \\tag{9.12} \\end{equation}\\] with \\(t(x)\\) the total of the ancillary variable, which must be known. This is a general estimator that can be used for any sampling design, not only for simple random sampling. For simple random sampling the population means of \\(z\\) and \\(x\\) are estimated by the sample means. For simple random sampling the sampling variance of the ratio estimator of the population total can be approximated by \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{t}_{\\mathrm{ratio}}(z)\\right)=N^2\\frac{\\widehat{S^{2}}(e)}{n} \\;, \\tag{9.13} \\end{equation}\\] with \\(\\widehat{S^{2}}(e)\\) the estimated variance of the residuals \\(e=z-bx\\): \\[\\begin{equation} \\widehat{S^{2}}(e)=\\frac{1}{(n-1)}\\sum\\limits_{i=1}^{n}e_i^2 \\tag{9.14} \\end{equation}\\] For simple random sampling without replacement from finite populations Equation (9.13) must be multiplied by \\((1-\\frac{n}{N})\\). Figure 9.4: Sampling distribution of ratio and Horvitz-Thompson estimator for SI of size 50 from Kandahar The average of the 10,000 ratio estimates of the total poppy area equals 3.615410^{4}. The population total of poppy equals 3.629510^{4}, so the estimated bias of the ratio estimator equals equals-140.565.The variance of the 10,000 ratio estimates equals 1.230456510^{8}, and the average of the 10,000 estimated approximate variances equals 1.090014310^{8}. The gain in precision due to the ratio estimator, quantified by the ratio of the variance of the Horvitz-Thompson estimator to the variance of the ratio estimator equals 1.698. 9.2.1 Ratio estimator for stratified simple random sampling Similar to the regression estimator there are two options: either estimate the ratios separately for the strata, or estimate a combined ratio. The separate ratio estimator is \\[\\begin{equation} \\hat{t}_{\\mathrm{ratio,s}}(z)=\\sum_{h=1}^H \\hat{t}_{h,\\mathrm{ratio}}(z) \\tag{9.15} \\end{equation}\\] with \\[\\begin{equation} \\hat{t}_{h,\\mathrm{ratio}}(z)=\\frac{\\hat{t}_h(z)}{\\hat{t}_h(x)} t_h(x) \\tag{9.16} \\end{equation}\\] in which \\(\\hat{t}_h(z)\\) is the estimated total of the study variable for stratum \\(h\\). The combined ratio estimator is \\[\\begin{equation} \\hat{t}_{\\mathrm{ratio,c}}(z)=\\frac{\\sum_{h=1}^H\\hat{t}_h(z)}{\\sum_{h=1}^H\\hat{t}_h(x)} t(x) \\tag{9.17} \\end{equation}\\] 9.3 Regression estimator with unknown mean of covariate The regression estimator (9.1) requires that the population mean of the ancillary variable \\(x\\) is known. This section is about applying the regression estimator in situations where this mean of \\(x\\) is unknown. A possible application is estimating the soil carbon stock (until a given depth) in an area. To estimate this carbon stock soil samples are collected and analyzed in laboratory. These laboratory measurements are very precise, but also expensive. Proximal sensors can be used to derive soil carbon concentrations from the spectra. Compared to laboratory measurements of soil these proximal sensor determinations are much cheaper, but also less precise. If there is a relation between the laboratory and proximal sensing determinations of SOC, then we expect that the regression estimator of the carbon stock will be more precise than the Horvitz-Thompson estimator which does not exploit the proximal sensing measurements. However, the spatial mean of the proximal sensing determinations is unknow. What we can do is estimate this mean from a large sample. At a subsample of this large sample, SOC concentration is also measured in the laboratory. This type of sampling is referred to as two-phase sampling. The term `phase’ does not refer to a period of time; all data can be collected in one sampling campaign. Intuitively with two-pase sampling the variance of the regression estimator of the total carbon stock will be larger than when the population mean of the proximal sensing determinations would be known. We are more uncertain about the total carbon stock, because we are uncertain about the population mean of the proximal sensing determinations. Figure 9.5 shows a two-phase sample from the cotton farm in Uzbekistan. In the first phase 40 points are selected by simple random sampling. In the second phase a subsample of 20 points (the triangles in the plot) are selected from the 40 points by simple random sampling without replacement. At all 40 points of the first-phase sample the covariate EM is measured, whereas the study variable (ECe) is measured at the 20 subsample points only. This sampling design does not require a full-coverage map of EM, which saves costs of fieldwork. #set sample sizes n1 &lt;- 80 n2 &lt;- 40 ids&lt;-sample.int(nrow(grdUzbekistan),size=n1,replace=FALSE) mysample &lt;- grdUzbekistan[ids,] #subsample the selected sample, and observe the study variable ids2 &lt;- sample.int(nrow(mysample), size = n2, replace = FALSE) mysubsample&lt;-mysample[ids2,] Figure 9.5: Two-phase sample for regression estimator of mean ECe Estimation of the spatial mean or total by the regression estimator from a two-phase sample is very similar to estimation when the covariate mean is known, as described above. The observations on the subsample can be used to estimate the regression coefficients \\(b\\). The true spatial mean of the ancillary variable, (\\(\\bar{x}\\)), is unknown now. This true mean is replaced by the mean as estimated from the relatively large first-phase sample. The estimated mean \\(\\hat{\\bar{x}}_{\\mathrm{SI}}\\) is estimated from the subsample. This leads to the following estimator \\[\\begin{equation} \\hat{\\bar{z}}_{\\text{regr,2ph}}= \\bar{z}_{s2}+b\\left( \\bar{x}_{s1}-\\bar{x}_{s2}\\right) \\;, \\tag{9.18} \\end{equation}\\] where \\(\\bar{z}_{s2}\\) is the subsample mean of the study variable, and \\(\\bar{x}_{s1}\\) and \\(\\bar{x}_{s2}\\) are the means of the covariate in the first-phase sample and subsample (second-phase sample), respectively. The sampling variance is larger than that of the regression estimator with known mean of \\(x\\). The variance can be decomposed into a component equal to the sampling variance of the estimated mean of \\(z\\), as estimated from the first-phase sample (supposing that the study variable would be observed on all units of the first-phase sample), and a component equal to the sampling variance of the regression estimator of the mean of \\(z\\) in the first-phase sample: \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\mathrm{regr,2ph}}\\right)=\\frac{\\widehat{S^{2}}(z)}{n_1} + (1-\\frac{n_2}{n_1}) \\frac{\\widehat{S^{2}}(e)}{n_2} \\;, \\tag{9.19} \\end{equation}\\] with \\(\\widehat{S^{2}}(e)\\) the variance of the regression residuals as estimated from the subsample \\[\\begin{equation} \\widehat{S^{2}}(e)=\\frac{1}{(n_2-2)}\\sum\\limits_{i=1}^{n}\\left(e_{i}\\right)^{2} \\tag{9.20} \\end{equation}\\] Note the finite population correction (fpc) \\((1-n_2/n_1)\\) in the variance estimator. This fpc accounts for the reduced variance due to subsampling the first-phase sample without replacement. Figure 9.6: Sampling distribution of regression estimator for two-phase sampling and regression estimator with mean of covariate known I repeated the two-phase sampling 10,000 times, see Figure 9.6. The variance of the regression estimator equals 0.0461. This is larger than for the regression estimator with known mean of the covariate EM (0.0367), but still smaller than the variance of the Horvitz-Thompson estimator (0.0554). Exercise (RegressionEstimator_Twophase.R) Write an R script to select a simple random sample without replacement of 80 points and a subsample of 40 points by the same sampling design from the cotton farm in Uzbekistan. Estimate the population mean of ECe by the regression estimator for two-phase sampling using EM as a covariate (EM is observed at all 80 points, ECe at the subsample only) Approximate the sampling variance of the regression estimator Repeat this 10,000 times in a for-loop, and compute the variance of the 10,000 regression estimates of the mean of ECe Compare the variance of the regression estimator for this two-phase sampling design with the variance of the regression estimator with known population mean of EM. "],
["Balanced.html", "Chapter 10 Balanced sampling 10.1 Balanced sample versus balanced sampling design 10.2 Unequal inclusion probabilities", " Chapter 10 Balanced sampling Similar to the regression estimator, balanced sampling is a sampling method that exploits one or more quantitative covariates that are related to the variable of interest. The idea behind balanced sampling is that, if we know the mean of the covariates, then the sampling efficiency can be increased by selecting a sample whose averages of the covariates is equal to the population means of the covariates. The simulated population Figure 10.1 shows a linear trend from West to East. In other words, the simulated study variable is correlated with the covariate Easting. To estimate the population mean of the simulated study variable, intuitively it is attractive to select a sample with an average of the Easting coordinate that is equal to the population mean of Easting (which is 10). Figure 10.1(a) shows such a sample; we say that the sample is `balanced’ on the covariate Easting. Figure 10.1: Sample balanced on Easting (a) and on Easting and Northing (b) 10.1 Balanced sample versus balanced sampling design We must distinguish a balanced sample from a balanced sampling design. A sampling design is balanced on a covariate \\(x\\) when all possible samples that can be generated by the design are balanced on \\(x\\). So, simple random sampling is not a balanced sampling design, because for many simple random samples the sample average of \\(x\\) is not equal to the population mean of \\(x\\). Only the expectation of the sample average of \\(x\\), i.e. the mean of the sample average over an infinite number of simple random samples, equals the population mean of \\(x\\). Figure 10.2 shows for one thousand simple random samples the squared error of the estimated population mean of the study variable \\(z\\) against the difference between the sample mean of \\(x\\) and the population mean of \\(x\\). Figure 10.2: Squared error in estimated mean of \\(z\\) against difference between population and sample mean of a covariate Clearly, the larger the absolute value of the difference, the larger on average the squared error. So to obtain a precise estimate of the population mean of \\(z\\), we better select samples with a difference close to 0. Sampling designs can also be balanced on multiple covariates. Figure 10.1(b) shows a sample balanced on both Easting and Northing. Using Easting as a balancing variable reduced the sampling variance of the estimated mean substantially, see Table 10.1. Using Northing as a second balancing variable further reduced the sampling variance. Table 10.1: Sampling variances of estimated mean for SI and balanced sampling of four units Sampling design Balancing variables Sampling variance SI - 39.70 Balanced Easting 14.40 Balanced Easting+Northing 9.77 10.2 Unequal inclusion probabilities Until now I assumed that the inclusion probabilities of the population units are equal, but this is not a requirement for balanced sampling designs. A more general definition is: a sampling design is balanced on variable \\(x\\) when for all samples generated by the design the Horvitz-Thompson estimator of the population mean of \\(x\\) equals the population mean: \\[\\begin{equation} \\frac{1}{N} \\sum_{k=1}^{n} \\frac{x_k}{\\pi_k}= \\frac{1}{N} \\sum_{k=1}^{N} x_k \\tag{10.1} \\end{equation}\\] In this equation \\(n\\) is the sample size, \\(\\pi_k\\) is the inclusion probability of unit \\(k\\), and \\(N\\) is the total number of units in the population. Similar to the regression estimator, balanced sampling exploits the linear relation between the variable of interest and one or more covariates. In the regression estimator this is done at the estimation stage. Balanced sampling does so at the sampling stage. For a single covariate the regression estimator equals (see Equation (9.1)) \\[\\begin{equation} \\hat{\\bar{z}}_{\\mathrm{regr}} = \\hat{\\bar{z}}_{\\mathrm{HT}} + b(\\bar{x} - \\hat{\\bar{x}}_{\\mathrm{HT}}) \\;, \\tag{10.2} \\end{equation}\\] with \\(\\hat{\\bar{z}}_{\\mathrm{HT}}\\) and \\(\\hat{\\bar{x}}_{\\mathrm{HT}}\\) the Horvitz-Thompson estimators of the mean of the study variable \\(z\\) and the covariate \\(x\\), respectively, \\(\\bar{x}\\) the population mean of the covariate, and \\(b\\) the estimated slope of the regression line. With a perfectly balanced sample the adjustment term in the regression estimator (the second term) equals zero. Balanced samples can be selected with the cube algorithm Deville and Tillé (2004), see also Brus (2015) for a detailed description of this algorithm. The population mean can be estimated by the Horvitz-Thompson estimator: \\[\\begin{equation} \\hat{\\bar{z}}_{\\mathrm{bal}} = \\frac{1}{N}\\sum_{i=1}^n \\frac{z_i}{\\pi_i}. \\tag{10.3} \\end{equation}\\] So with equal inclusion probabilities, equal to \\(n/N\\), the mean is estimated by the sample mean of the study variable. The variance of the estimated mean can be approximated by (Grafström and Tillé 2013) \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{bal}}\\right)=\\frac{1}{N}\\frac{n}{n-p}\\sum_{i=1}^n (1-\\pi_i)\\left(\\frac{e_i}{\\pi_i}\\right)^2\\;, \\tag{10.4} \\end{equation}\\] with \\(p\\) the number of balancing variables, and \\(e_i = z_i - \\mathbf{x}_i^{\\text{T}}\\hat{\\boldsymbol{\\beta}}\\) the residual of unit \\(i\\), with \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=\\left[\\sum_{i=1}^n(1-\\pi_i)\\frac{\\mathbf{x}_i}{\\pi_i}\\frac{\\mathbf{x}^{\\text{T}}_i}{\\pi_i}\\right]^{-1}\\sum_{i=1}^n(1-\\pi_i)\\frac{\\mathbf{x}_i}{\\pi_i}\\frac{y_i}{\\pi_i} \\tag{10.5} \\end{equation}\\] Working this out for simple random sampling without replacement (\\(\\pi_i = n/N\\) for \\(i = 1 \\cdots N\\)) gives \\[\\begin{equation} \\widehat{V}\\!\\left(\\hat{\\bar{z}}_{\\text{bal}}\\right)=\\left( 1-\\frac{n}{N} \\right) \\frac{1}{n}\\sum_{i=1}^n e_i^2\\;, \\tag{10.6} \\end{equation}\\] With his design the residuals \\(e_i\\) can be computed by fitting a linear regression model by ordinary least squares. For small sampling fractions \\(n/N\\) and for balanced sampling from infinite poplations the finite population correction can be dropped from this variance estimator. Balanced sampling is now illustrated with the field of simulated ECe values on the cotton research farm in Uzbekistan. EM is used as balancing variable. library(sampling) #compute population size N&lt;-nrow(grdUzbekistan) #set sample size n&lt;-40 #define matrix with covariate for balancing; first column of matrix must be filled with ones X&lt;-cbind(rep(1,times=N),grdUzbekistan$EM) #compute inclusion probabilities; use equal probabilities pik&lt;-rep(n/N,times=N) sampleind=samplecube(X=X,pik=pik,comment=FALSE,method=1) mysample&lt;-grdUzbekistan[sampleind==1,] #estimate population mean estimatedMean &lt;- mean(mysample$ECe) #estimate variance of estimated mean lmsample&lt;-lm(ECe~EM,data=mysample) e&lt;-residuals(lmsample) S2e&lt;-sum(e^2)/(n-2) estimatedVarofMean&lt;-S2e/n Figure 10.3 shows the result. The sample mean of EM equals 48.9. The population mean of EM equals 49.9. Figure 10.3: Balanced sample from the cotton research farm (Uzbekistan), balanced on covariate EM Figure 10.4 shows the sampling distributions of the estimated mean of ECe for balanced sampling and simple random sampling, obtained by repeating the random sampling with both designs and estimation 1000 times. Figure 10.4: Sampling distribution of estimated mean of ECe for balanced sampling and SI The empirical variance of the estimated population mean of the study variable ECe equals 0.035. The gain in precision compated to simple random sampling equals 1.4. The mean of the 1000 estimated variances equals 0.035, showing that the approximate variance estimator performs well in this case. The sample mean of EM varies between the samples (Figure 10.5). The population mean of the balancing variable EM equals 49.9. In other words, many samples are not perfectly balanced on EM. This is not exceptional, in most cases perfect balance is impossible. Figure 10.5: Sampling distribution of sample mean of balancing variable EM 10.2.1 Balanced sampling with geographical spreading When a sample is balanced on a covariate, this does not necessarily imply a good spread along the axis spanned by the covariate. Balancing and spreading of sampling locations are different things. For that reason I do not like the term spatially balanced sampling for sampling with spreading in geographical space. I prefer the term spatial coverage sampling for this. Grafström and Tillé (2013) presented a method for selecting balanced samples that are also well-spread along the axes of one or more covariates. If we take for the spreading covariates Easting and Northing, this leads to balanced samples with good spatial coverage (balanced spatial coverage samples). when the residuals of the regression mdeol show spatial structure (are spatially correlated), the estimated population mean of the study variable becomes more precise thanks to the improved geographical spreading. Balanced samples with spreading can be selected with function lcube of R package BalancedSampling. library(BalancedSampling) sampleid=lcube(Xbal=X,Xspread=X,prob=pik) mysample&lt;-grdUzbekistan[sampleid,] Figure 10.6: Balanced spatial coverage sample from the cotton research farm (Uzbekistan), balanced on EM "],
["IntroSamplingforMapping.html", "Chapter 11 Sampling for mapping 11.1 Geometric versus model-based sampling designs", " Chapter 11 Sampling for mapping This second part of the book deals with sampling for mapping. In practice a map is constructed by overlaying the study area by a very fine square grid, and estimating the variable of interest for all grid cells. Note that the estimated value may either represent the mean of the grid cell or the value at its centre. We should be clear about this. For mapping a model-based sampling approach is the only option. We cannot afford to select a random sample from each grid cell for design-based estimation of their means. A statistical model, containing an error term modeled by a probability distribution, is used to map the study variable from the sample data. As the model already contains a random error, selection of the sampling units by probability sampling is not strictly needed in a model-based approach. As a consequence there is room for optimizing the sampling units, i.e. searching for those units that lead to the most precise map. As an illustration, consider the following model: \\[\\begin{equation} z_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\;, \\tag{11.1} \\end{equation}\\] with \\(z_i\\) the study variable of unit \\(i\\), \\(\\beta_0\\) and \\(\\beta_1\\) regression coefficients and \\(\\epsilon_i\\) the error (residual) at unit \\(i\\), normally distributed with mean zero and a constant variance \\(\\sigma^2\\). The errors are assumed independent, so that \\(Cov(\\epsilon_i,\\epsilon_j)=0\\) for all \\(i \\neq j\\). Figure 11.1 shows a simple random sample without replacement (subfigure a) and the sample optimized for mapping with a simple linear regression model (subfigure b). Both samples are plotted in a map of the covariate (predictor). Figure 11.1: Simple random sample (a), optimized samples for mapping with simple linear regression (b) and kriging with an external drift (c), and stratified sample using sixteen equal-sized covariate strata (d) The optimized sample for mapping with a simple linear regression model contains the units with the smallest or the largest values of the covariate \\(x\\). The optimized sample shows strong spatial clustering. Spatial clustering is not avoided because in a simple linear regression model we assume that the data are independent. The standard errors of both regression coefficients are considerably smaller for the optimized sample (Table 11.1). The joint uncertainty about the two regression coefficients, quantified by the determinant of the variance-covariance matrix of the estimated regression coefficients, equals 0.0020 for the simple random sample and 0.00010 for the optimized sample. When we are less uncertain about the regression coefficients, we are also less uncertain about the estimates of the study variable obtained with the simple linear regression model at (unobserved) locations of a fine grid discretizing the study area used for mapping the study variable. So, we conclude that for mapping with a simple linear gression model, simple random sampling is not a good option. Table 11.1: Standard errors and determinant of variance-covariance matrix of estimated regression coefficients with simple random sample and optimized sample Sampling design se intercept se slope Determinant SI 1.51 0.086 2e-03 Optimized 1.08 0.051 1e-04 The following model differs slightly from Equation (11.1): \\[\\begin{eqnarray} y_i &amp;=&amp; \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\ \\epsilon_i &amp;\\sim&amp; \\mathcal{N}(0,\\sigma^2)\\\\ \\mathrm{cov}(\\epsilon_i,\\epsilon_j) &amp;=&amp; C(d_{i,j}) \\;. \\tag{11.2} \\end{eqnarray}\\] In this model the errors are not independent, but spatially correlated. This model is used when mapping by kriging with an external drift (KED). Figure 11.1(c) shows an optmized sample for KED mapping4. Spatial clustering is avoided, the selected units are spread throughout the area. At the same time units near the minimum (unit with coordinates (13.5, 12.5)) and maximum (unit with coordinates (13.5, 6.5)) of the covariate are selected. This example shows that there is no single best sampling design for mapping. The best design depends on the method used for mapping. If we believe that the study variable can better be mapped by KED instead of simple linear regression, because we expect the data to be spatially autocorrelated, the optimal sample largely differs from the optimal sample for mapping using a simple linear regression model. If we foresee a quadratic relation, \\(z_i = \\beta_0 + \\beta_1x_i + \\beta_2x^2_i+\\epsilon_i\\), the optimal sample will also include locations with covariate values near the mean of \\(x\\). And if we expect an even more complicated, non-linear relation, it can be advantageous to select a sample so that the relative frequency distribution of \\(x\\) in the sample and in the population are similar. This can be done by using the covariate \\(x\\) to construct as many strata as the number of sampling unts we want to select. By using evenly spaced quantiles of \\(x\\) as stratum breaks, the strata have equal size. From each stratum one unit is selected (Figure 11.1(d)). This is the rationale of conditioned Latin hypercube sampling in case we have multiple covariates (Chapter 15). 11.1 Geometric versus model-based sampling designs At the highest level we may distinguish geometric from model-based sampling designs for mapping. A square grid and a triangular grid are examples of geometric sampling designs; the sampling locations show a regular, geometric spatial pattern. In other geometric sampling designs the spatial pattern is not that nicely regular. Yet these are classified as geometric sampling designs when the samples are obtained by minimizing a geometric criterion, i.e. a criterion defined in terms of distances between the sampling locations and the nodes of a fine prediction grid discretizing the study area (Chapters 13 and 14). In model-based sampling designs the samples are obtained by minimizing a criterion that is defined in terms of variances of prediction errors. An example is the mean kriging variance criterion, i.e. the average of the kriging variances over all nodes of the prediction grid. Model-based sampling therefore requires some knowledge of the model of spatial variation. Such a model must be postulated, and given this model the sample can be optimized. In Chapter 1 we introduced the design-based and model-based approach for sampling and statistical inference. Note that a model-based approach does not necessarily imply model-based sampling. The adjective model-based refers to the model-based inference, not to the selection of the locations. In a model-based approach sampling locations can be, but need not be selected by model-based sampling. If they are, then both in selecting the locations and in mapping a statistical model is used. In most cases the two models differ: once the sample data are collected, these are used to update the postulated model used for sampling design. This updated model is then used for mapping. Which sample is optimal for KED also depends on how strong the spatial correlation is, see Chapter ??↩ "],
["Regulargrid.html", "Chapter 12 Sampling on a regular grid", " Chapter 12 Sampling on a regular grid For mapping sampling on a regular grid is attractive because of its simplicity. The data collected on the grid-points are not used for design-based estimation of the population mean or total, and for that reason the grid need not be placed randomly on the study area as in systematic random sampling (Chapter 5). The grid can be located such that the grid nodes optimally cover the study area. Commonly used grid configuration are square, triangular and hexagonal. Which configuration is optimal depends amongst others on the variogram. If the variable of interest shows moderate to strong spatial autocorrelation, triangular grids give the best result. Besides the shape of the grid cells, we must decide on the size of the grid cells (grid spacing). The grid spacing determines the number of sampling locations in the study area, the sample size. There are two options to decide on this spacing, either by starting from the available budget or from a requirement on the quality of the map. The latter is explained in Chapter ??, as this requires a model of the spatial variation, and as a consequence this is an example of model-based sampling. Starting from the available budget and an estimate of the costs per point, we first compute the affordable sample size. Then we may derive from this number the grid spacing. For square grids, the grid spacing in meters can be calculated with \\(\\sqrt{A/n}\\), where \\(A\\) is the area in m\\(^2\\), and \\(n\\) is the number of sampling locations (sample size). Grids can be selected with function spsample of R package sp (Pebesma and Bivand 2005). The argument offset is used to select non-randomly a grid. Either a sample size can be specified, using argument n, or a gridspacing using argument cellsize. gridded(grdVoorst) &lt;- ~s1+s2 mysample&lt;-spsample(x=grdVoorst,type=&quot;regular&quot;,cellsize=c(200,200),offset=c(0.5,0.5)) mysample&lt;-as(mysample,&quot;data.frame&quot;) Figure 12.1 shows the selected square grid. Figure 12.1: Non-random square grid sample from Voorst The number of gridpoints equals 115. Nodes of the square grid in parts of the area not belonging to the population of interest, such as build-up areas and roads, are discarded. As a consequence, there are some undersampled areas, for instance in the middle of the study area where two roads cross. If we use the square grid in spatial interpolation, e.g. ordinary kriging, we are more uncertain about the predictions in these undersampled areas than in areas where the grid is complete. In the next Chapter I show how this local undersampling can be avoided. "],
["SpatialCoverage.html", "Chapter 13 Spatial coverage sampling 13.1 Spatial infill sampling", " Chapter 13 Spatial coverage sampling Local undersampling with regular grids can be avoided by relaxing the constraint that the sampling locations are restricted to the nodes of a regular grid. This is what is done in spatial coverage sampling or, in case of a sample that is added to an existing sample, in spatial infill sampling. Spatial coverage and infill samples cover the area or fill in the empty space as uniformly as possible. The sampling locations are obtained by minimizing a criterion that is defined in terms of the geograhic distances between the nodes of a fine interpolation grid and the sampling locations. Brus, Gruijter, and Groenigen (2007) proposed to minimize the mean of the squared distances of the grid nodes to their nearest sampling location (Mean Squared Shortest Distance): \\[\\begin{equation} MSSD=\\frac{1}{N}\\sum_{i=1}^{N}\\min_{j}\\left(D_{ij}^{2}\\right) \\;, \\tag{13.1} \\end{equation}\\] where \\(N\\) is the total number of nodes of the interpolation grid, and \\(D_{ij}\\) is the distance between the \\(i\\)th grid node and the \\(j\\)th sampling location. This distance measure can be minimized by the k-means algorithm. The same algorithm is used in Chapter 4 to construct compact geographical strata (shortly referred to as geostrata) for stratified random sampling. In stratified random sampling, one or more sampling locations are selected randomly from each geostratum. For mapping purposes probability sampling is not required. For each geostratum the means of the spatial coordinates of the nodes of that geostratum are calculated, and these centroids are used as sampling locations. This improves the spatial coverage compared to stratified random sampling. In probability sampling we may want to have strata of equal area (clusters with equal numbers of nodes), so that the sampling design becomes self-weighting. For mapping this constraint is not recommended as it may lead to samples with suboptimal spatial coverage. Existing sampling locations can easily be accommodated in the k-means algorithm, by using them as fixed centroids. Spatial coverage and infill samples can be computed with R package spcosa (Walvoort, Brus, and Gruijter 2010). Figure 13.1 shows a spatial coverage sample of the same size as the regular grid in study area Voorst (Figure 12.1). Note that the undersampled area in the centre of the study area is now covered by a sampling location. #set number of sampling locations to be selected n&lt;-115 set.seed(314) #compute compact geostrata gridded(grdVoorst) &lt;- ~s1+s2 myStrata &lt;- stratify(grdVoorst, nStrata = n, equalArea=FALSE, nTry=10) #select the centres of the geostrata mySample &lt;- spsample(myStrata) #plot geostrata and sampling points (centres of geostrata) plot(myStrata, mySample) Figure 13.1: Spatial coverage sample from Voorst 13.1 Spatial infill sampling If georeferenced data are available that can be used for mapping the study variable, but we need more data for mapping, it is attractive to account for these existing sampling locations when selecting the additional sampling locations. The aim now is to fill in the empty spaces, i.e. the parts of the study area not covered by the existing sampling locations. This is referred to as spatial infill sampling. Figure 13.2 shows a spatial infill sample for three woredas in Ethiopia. A large set of `legacy’ data on soil organic carbon (SOC) concentration in the topsoil is available, but these data come from strongly spatially clustered locations along roads. This is a nice example of a convenience sample. The legacy data are not ideal for mapping SOC throughout the three woredas. Preferably, additional data are collected in the interior, off-road parts of the woredas, with the exception of the notheastern part where we have already quite a few data. gridded(grdEthiopia)&lt;-~s1+s2 #Set number of new sampling locations to be selected n&lt;-100 #Compute total sample size (existing points + new points) ntot&lt;-n+length(priordataEthiopia) #Change class of d (existing points) from SpatialPointsDataFrame to SpatialPoints priordataEthiopia&lt;-as(priordataEthiopia,&quot;SpatialPoints&quot;) #Remove projection attributes of priordataEthiopia proj4string(priordataEthiopia)&lt;- NA_character_ #Compute geostrata with argument priorPoints set.seed(314) myStrata &lt;- stratify(grdEthiopia, nStrata = ntot, priorPoints=priordataEthiopia, nTry=10) #Select sampling points of infill sample (centres of geostrata) mySample &lt;- spsample(myStrata) #Plot geostrata and sampling points (centres of geostrata) plot(myStrata, mySample) Figure 13.2: Spatial infill sample in three woredas of Ethiopia #Select the new points from mySample ids &lt;- which(mySample@isPriorPoint==F) #Change class of mySample to data.frame mySample &lt;- as(mySample,&quot;data.frame&quot;) mySamplenew &lt;- mySample[ids,] Exercise (SpatialInfill.R) Write an R script to select a spatial infill sample of size 100 for the three woredas in Ethiopia. Existing data are in the SpatialPointsDataFrame priordataEthiopia stored in DataThreeWoredasEthiopia.RData. The data.frame grdEthiopia is stored in CovariatesThreeWoredasEthiopia.RData Compute the actual number of selected new locations (size of infill sample). How comes that more than 100 additional locations are selected? See the warning of function stratify Change the total number of locations (number of existing + new locations) so that exactly 100 new locations are selected "],
["kmeans.html", "Chapter 14 K-means sampling 14.1 Hard k-means 14.2 Fuzzy k-means", " Chapter 14 K-means sampling Regular grid sampling and spatial coverage sampling are pure spatial sampling designs. Covariates possibly related to the study variable are not accounted for in selecting sampling units. This can be suboptimal when the study variable is related to covariates of which maps are available, think for instance of remote sensing imagery. These maps can then be used in mapping the study variable by, for instance, a multiple linear regression model. This chapter describes a simple, straightforward method for selecting sampling units on the basis of the covariate values of the grid cells. The sampling methods are illustated with a study area in the Hunter Valley, New South-Wales in Australia. Five quantitative covariates are available: elevation, slope, cosine of aspect, compound topographic index (cti) and normalized difference vegetation index (ndvi). The plot below shows maps of these five covariates. The sharp boundaries on the ndvi map coincide with boundaries of agricultural fields. Figure 14.1: Five covariates of study area Hunter Valley used in k-means sampling 14.1 Hard k-means In hard k-means sampling the covariates are used to cluster the grid cells by the k-means clustering algorithm. Similar to spatial coverage sampling (Chapter 13 the MSSD is minimized, but now the distance is not measured in geographical space but in a \\(p\\)-dimensional space spanned by the \\(p\\) covariates (think of it as a multi-dimensional scatter plot with the covariates along the axes). The covariates are scaled so that their standard deviations become 1. This is needed because, contrary to the x- and y-coordinates used as clustering variables in spatial coverage sampling, the dimensions of the covariates used as clustering variables generally differ, and the ranges of the covariates in the population can largely differ. In the clustering of the grid cells the Mean Squared Shortest Scaled Distance (MSSSD)5 is minimized. In the next chunk a hard k-means sample of size 20 is selected from the Hunter Valley study area. All five quantitative covariates are used as covariates. To select 20 points 20 clusters are constructed using hard k-means. Note that the number of clusters is based on the required sample size, not as usual in cluster analysis on the number of subregions with a high density of points in the multivariate distribution. Grid cells with the shortest scaled Euclidean distance in covariate-space to the centroids of the clusters are selected as the sampling points. #Set number of sampling locations to be selected n&lt;-20 #Compute clusters set.seed(314) myClusters &lt;- kmeans(scale(grdHunterValley[,c(3,4,5,6,7)]), centers=n, iter.max=1000,nstart=10) grdHunterValley$cluster &lt;- myClusters$cluster #Select locations closest to the centers of the clusters rdist.out &lt;- rdist(x1=myClusters$centers,x2=scale(grdHunterValley[,c(3,4,5,6,7)])) units &lt;- apply(rdist.out,MARGIN=1,which.min) myKMSample &lt;- grdHunterValley[units,] Figure 14.2 shows the clustering of the grid cells and their centroids used as the selected hard k-means sample. In Figure 14.3 the selected sample is plotted in a scatter diagram of cti against ndvi. The MSSSD of the selected sample is computed by: populationmeans &lt;- apply(grdHunterValley[,c(3,4,5,6,7)],MARGIN=2,FUN=mean) populationsds &lt;- apply(grdHunterValley[,c(3,4,5,6,7)],MARGIN=2,FUN=sd) rdist.out &lt;- rdist(x1=scale(grdHunterValley[,c(3,4,5,6,7)]),x2=scale(myKMSample[,c(3,4,5,6,7)],center=populationmeans,scale=populationsds)) dmin &lt;- apply(rdist.out,MARGIN=1,min) MSSSD &lt;- mean(dmin^2) The MSSSD of the selected sample equals 1.106. 14.2 Fuzzy k-means In hard k-means each unit (grid cell) can only belong to exactly one cluster. Fuzzy k-means (also referred to as soft k-means) allows units to belong to one or more clusters. A vector containing \\(k\\) numbers is assigned to every unit, with all numbers in the interval [0,1]. The numbers sum to 1. The numbers indicate the degree to which a unit belongs to each cluster. They are referred to as membership grades. With fuzzy k-means, the centroid of a cluster is the weighted mean of the covariates over all units, using the memberships of that cluster as weights. As before, grid cells with the shortest Euclidean distance in covariate-space to the centroids of these fuzzy clusters are selected as the sampling points. Fuzzy k-means clustering can be done with function FKM of package fclust and function runFuzme of R package fuzme. R package fuzme can also be used for clustering using Mahalanobis distances. Clustering using Mahalanobis distances can also be achieved with function fanny of R package cluster. My experience is that computing time with these R packages is prohibitive when we have a large number of grid cells. In that case I recommend the software FuzME. In the next chunk a sample of size 20 is selected, using the memberships obtained with FuzME as input. In fuzzy k-means clustering a fuzziness exponent of 1.3 is used. The covariates are scaled. Correlation between the covariates is not accounted for (whcih can be done by selecting Mahalanobis distance as metric distance in FuzME). # read memberships computed with FuzMe m &lt;- read.csv(file=&quot;20_class_Diag.txt&quot;,sep=&quot;&quot;) m &lt;- m[,-c(1,2,3)] # defuzzify, i.e. compute for each gridcell the cluster with largest membership grdHunterValley$defuzcluster &lt;- apply(m,MARGIN=1,which.max) #select locations with largest membership in cluster 1...k units &lt;- apply(m,MARGIN=2,FUN=which.max) myFKMSample &lt;- grdHunterValley[units,] Figure 14.2 shows the selected fuzzy k-means sample in geographical space. The clustering is defuzzified, i.e. for each grid cell the cluster with the largest membership is assigned to this grdicell. Figure 14.3 shows the selected fuzzy k-means sample plotted in the same scatter diagram as the k-means sample. Figure 14.2: Hard k-means and fuzzy k-means sample from study area Hunter Valley, using five covariates in clustering Figure 14.3: Hard k-means and fuzzy k-means sample plotted in scatter diagram of compound topographic index against normalized difference vegetation index Exercise (KMSample.R) Write an R script to select a k-means sample of size 20, using the five scaled covariates of Hunter Valley in clustering the grid cells Compute the Mean Squared Shortest Scaled Distance (MSSSD) of the k-means sample Use FuzME to compute a fuzzy clustering for 20 classes,. The input file for FuzME is grdHunterValley4Practicals4FuzME.csv. Use scaled distances (in FuzME, in box Metric Distance tic Diagonal). Use 1.3 as fuzzy exponent, and set maximum number of iterations to 1000 Write an R script to select the fuzzy k-means sample for this fuzzy clustering, and compute the MSSSD Repeat this for Mahalanobis distances Compare the MSSSD’s of the k-means and the two fuzzy k-means samples The name scaled distance can be confusing. Not the distances are scaled, but the distances are computed in a space spanned by the scaled covariates↩ "],
["cLHS.html", "Chapter 15 Conditioned Latin hypercube sampling", " Chapter 15 Conditioned Latin hypercube sampling This chapter describes an experimental design that has been adapted for spatial surveys. An adaptation is necessary because in contrast to experiments in observational studies one is not free to choose combinations of levels of different factors. When two covariates are strongly correlated it may happen that there are no locations with a relatively large value for one covariate and a relatively small value for the other covariate. In experimental research it is possible to select combinations of levels of factors so that the factors are independent. In a full factorial design all combinations of factor levels are observed. With \\(k\\) factors and \\(l\\) levels per factor the total number of observations is \\(l^k\\) . With numerous factors and/or numerous levels per factor this becomes unfeasible in practice. Alternative experimental designs have been developed that need less observations but still provide detailed information about how the variable of interest responds to changes in the factor levels. In this chapter I describe and illustrate the survey sampling analogue of Latin hypercube sampling. Response surface sampling will follow in a next chapter. Latin hypercube sampling (LHS) is used in designing (computer) experiments with numerous covariates and/or factors of which we want to study the effect on the output (McKay, Beckman, and Conover {1979}). Suppose we have only two covariates, e.g. application rates for N and P in agricultural experiment, and 4 levels for each covariate. It is evident that the best option is to have multiple plots for all \\(4 \\times 4\\) combinations. This is referred to as a full factorial design. With numerous covariates and/or levels per covariate, this becomes unfeasible. A much cheaper alternative then is an experiment with, for all covariates, exactly one observation per level. So in the agricultural experiment this would entail four observations, distributed in a square in such way that we have in all rows and in all columns one observation. This is referred to as a Latin square. The generalisation of a Latin square to a higher number of dimensions is a Latin hypercube. Minasny and McBratney (2006) adapted LHS for observational studies; this adaptation is referred to as conditioned LHS (cLHS). For each covariate a series of intervals (marginal strata) is defined. The breaks of the marginal strata are chosen such that the numbers of grid cells in these marginal strata are equal. This can be done by using the quantiles corresponding with evenly spaced cumulative probabilities as stratum breaks. For instance, for five marginal strata we use the quantiles corresponding with the cumulative probabilities 0.2, 0.4, 0.6 and 0.8. Minasny and McBratney (2006) developed a search algorithm, based on heuristic rules and an annealing schedule, to select a cLHS (see for an explanation of annealing, Chapter ??). The objective function that is minimized is the weighted sum of three components: * O1: the sum over all marginal strata of the absolute difference between the marginal stratum sample size and the targeted sample size (equal to 1) * O2: the sum over all classes of categorical covariates of the absolute difference between the sample proportion of a given class and the population proportion of that class * O3: the sum over all entries of the correlation matrix of the absolute difference of the correlation in the population and in the sample With cLHS the marginal distributions of the covariates in the sample are close to these distributions in the population. This can be advantageous for mapping methods that do not rely on linear relations, for instance in machine learning techniques like classification and regression trees (CART), and random forests. cLH sampling is illustrated with the Hunter Valley data used before in k-means sampling (Chapter 14). To speed up the computations a subgrid is selected with a spacing of \\(50 \\times 50\\) m (the original cell size is \\(25 \\times 25\\) m). gridded(grdHunterValley) &lt;- c(&quot;Easting&quot;,&quot;Northing&quot;) subgrid &lt;- spsample(grdHunterValley,type=&quot;regular&quot;,cellsize=50,offset=c(0.5,0.5)) subgriddata &lt;- (subgrid %over% grdHunterValley) grd &lt;- data.frame(coordinates(subgrid),subgriddata) grdHunterValley &lt;- as(grdHunterValley,&quot;data.frame&quot;) cLHS samples can be selected with function optimCLHS of R package spsann (Samuel-Rosa 2016). First the candidate sampling points and the covariates are specified. library(spsann) candi &lt;- grd[,1:2] names(candi) &lt;- c(&quot;x&quot;,&quot;y&quot;) covars &lt;- grd[, 3:7] The next step is to define the annealing schedule. Note that both the initial acceptance rate and the initial temperature are set, which may seem weird as the acceptance rate is a function of the initial temperature: \\(P =e^{\\frac{-\\Delta f}{T}}\\). The initial acceptance rate is used as a threshold value. If an initial temperature is chosen that leads to an acceptance rate smaller than the chosen value for the initial acceptance rate, then the optimization stops. In this case a larger value for the initial temperature must be chosen. schedule &lt;- scheduleSPSANN(initial.acceptance = 0.8,initial.temperature = 0.05, temperature.decrease=0.95, chains=1000, chain.length=4, stopping=10, x.min=10,y.min=10, cellsize=50) Finally, specify the weights for \\(O1\\) and \\(O3\\) in the objective function, and start the simulated annealing algorithm. weights &lt;- list(O1 = 0.5, O3 = 0.5) samplesize&lt;-20 set.seed(314) optimCLHS.out &lt;- optimCLHS( points = samplesize, candi = candi, covars = covars, use.coords = FALSE, schedule = schedule, track=TRUE, weights = weights, progress=NULL) ## ## 81% of acceptance in the 1st chain ## running time = 25.77 seconds mycLHSample &lt;- data.frame(optimCLHS.out$points,grd[optimCLHS.out$points$id,3:7]) Figure 15.1 shows the selected sample in a map of compound topographic index, which is one of the five covariates used in sampling. Figure 15.1: Conditioned Latin hypercube sample from study area Hunter Valley in a map of compound topographic index, which is one of the five covariates used in sampling Figure 15.2 shows the sample plotted in a scatter diagram of cti against ndvi. The horizontal and vertical lines in this scatter diagram are at the boundaries of the marginal strata of cti and ndvi, respectively. The number of points within the horizontal and vertical bars are equal (marginal strata have equal size), which explains that the intervals are the narrowest where the density of points in the plot is highest. We can see in Figure 15.2, for instance, that in the first marginal stratum of cti there are two sampling locations, whereas in the 7th and 8th marginal ndvi strata no sampling locations are selected. Figure 15.2: Conditioned Latin hypercube sample plotted in scatter diagram of compound topographic index against normalized difference vegetation index Figure 15.3 shows the sample sizes for all 100 marginal strata. Ideally, all marginal strata contain one sampling unit. Figure 15.3: Sample sizes of marginal strata For all marginal strata with one sampling location the contribution to component O1 of the criterion is zero. For strata with zero or two sampling locations, the contribution is one. There is one marginal stratum, the 18th marginal stratum of aspect, that has three sampling locations, so that the contribution of this stratum to O1 equals 2. O1 &lt;- sum(abs(counts-1)) For the selected sample component O1 equals 24. In package spsann this number is divided by the total number of marginal strata, and multiplied by the weight. Fig 15.4 shows the trace of the objective function components O1 and O3, i.e. the values during the optimization. First the trace is extracted from the output of function optimCLHS. trace &lt;- optimCLHS.out$objective$energy Figure 15.4: Trace of O1 (black) and O3 (red) during optimization of conditioned Latin hypercube sampling from Hunter Valley The final value of O1 in the data.frame trace equals 0.12, which equals the value of O1 computed above as a sum, divided by the total number of marginal strata (100) and multiplied by the weight for O1 (0.5). (corpopulation &lt;- cor(grd[,c(3,4,5,6,7)])) ## elevation slope aspect cti ndvi ## elevation 1.00000000 0.76713639 0.01000011 -0.44371020 -0.08527735 ## slope 0.76713639 1.00000000 0.01941342 -0.48705948 0.01332772 ## aspect 0.01000011 0.01941342 1.00000000 -0.01011080 -0.01050931 ## cti -0.44371020 -0.48705948 -0.01011080 1.00000000 0.06137475 ## ndvi -0.08527735 0.01332772 -0.01050931 0.06137475 1.00000000 (corsample &lt;- cor(mycLHSample[,c(4,5,6,7,8)])) ## elevation slope aspect cti ndvi ## elevation 1.000000000 0.752011642 -0.005350369 -0.54025450 -0.098692778 ## slope 0.752011642 1.000000000 -0.005814333 -0.53490165 0.033692176 ## aspect -0.005350369 -0.005814333 1.000000000 0.02457119 -0.007660007 ## cti -0.540254501 -0.534901650 0.024571188 1.00000000 0.047632805 ## ndvi -0.098692778 0.033692176 -0.007660007 0.04763280 1.000000000 O3 &lt;- sum(abs(corpopulation-corsample)) Component O3 of the minimization criterion equals 0.57. Conditioned Latin hypercube samples can also be selected with R package cLHS (Roudier 2011). "],
["references.html", "References", " References "]
]
