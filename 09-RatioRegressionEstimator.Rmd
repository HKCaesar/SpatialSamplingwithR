# Regression and ratio estimator {#RegressionEstimator}

This Chapter is about the use of covariates in design-based estimation from probability samples. In many cases ancillary information is available that could be useful to increase the precision of the estimated mean or total of the study variable. The ancillary variable(s) can be qualitative (i.e., classifications) or quantitative. As we have seen before both types of ancillary variable can be used at the selection stage (design stage) to improve the performance of the sampling strategy, for instance by stratification (Chapter \@ref(STSI)) or selecting sampling units with probabilities proportional to size (Chapter \@ref(pps)). In this Chapet I explain how these covariates can be used at the stage of *estimation*.

## Regression estimator
Suppose we have observed along with the study variable a quantitative variable that is linearly related to the study variable. Further suppose that we know the population mean of this covariate. Then for simple random sampling the mean of the study variable can be estimated by the regression estimator:
\begin{equation}
\hat{\bar{z}}_{\text{regr}}= \bar{z}_s+b\left( \bar{x}-\bar{x}_s\right) \;,
(\#eq:RegressionEstimatorSI)
\end{equation}
where $\bar{z}_s$ is the sample mean of the study variable, $\bar{x}_s$ is the sample mean of the covariate, $\bar{x}$ is the population mean of the covariate, and $b$ is the estimated regression coefficient (estimated slope). Note that for this estimator the population mean of the covariate must be known. So, if we know the covariate values only at the sampling points, then this estimator cannot be used. The regression coefficient $b$ is estimated by the least squares estimator:
\begin{equation}
b=\frac{\sum_{i=1}^n(x_i-\bar{x}_s)(z_i-\bar{z}_s)}{\sum_{i=1}^n(x_j-\bar{x}_s)^2}
(\#eq:RegressionCoefficientSI)
\end{equation}
The rationale of the regression estimator is that when the estimated mean of the covariate is, for instance, smaller than the population mean of the covariate, then with a positive correlation between the study variable and covariate, also the estimated mean of the study variable is expected to be smaller than the population mean of the study variable. The difference between the population mean and estimated mean of the covariate can be used to improve the simple random sample estimate of the mean of $z$, by adding a term proportional to the difference between the estimated mean and population mean of the covariate. As a proportion the slope of the regression line is used.

The sampling variance of this regression estimator can be estimated by computing first the regression residuals $e_i= z_i - \hat{z}_i, i = 1 \cdots n$ at the sampling points. The sampling variance of the regression estimator is *approximately* equal to the sampling variance of the estimated mean of these residuals:
\begin{equation}
\widehat{V}\!\left(\hat{\bar{z}}_{\mathrm{regr}}\right)=\frac{\widehat{S^{2}}(e)}{n} \;,
(\#eq:VarianceRegressionEstimatorSI)
\end{equation}
with $\widehat{S^{2}}(e)$ the estimated population variance of the regression residuals
\begin{equation}
\widehat{S^{2}}(e)=\frac{1}{(n-2)}\sum\limits_{i=1}^{n}\left(e_{i}\right)^{2}
(\#eq:VarianceResiduals)
\end{equation}
The variance estimator is an approximation because the regression coefficient is also estimated from the sample, which makes the regression estimator nonlinear. For simple random sampling without replacement from finite populations, the variance estimator must be multiplied by $1-n/N$.

The regression estimator is illustrated with the electrical conductivity (ECe) data at the cotton farm in Uzbekistan, see Figure \@ref(fig:EMCottonFarm). As a covariate I use the electromagnetic measurements of the apparent electrical conductivity  (EM). Figure \@ref(fig:ExhaustiveScatter) shows a scatter plot for all grid nodes (referred to as the exhaustive scatter plot).

```{r ExhaustiveScatter, echo=F, out.width='70%', fig.asp=.6, fig.cap="Exhaustive scatter plot of ECe against EM "}
grdUzbekistan$EM <- exp(grdUzbekistan$lnEM)

lmpopulation<-lm(ECe~EM,data=grdUzbekistan)
ab<-coef(lmpopulation)

ggplot(data = grdUzbekistan) +
        geom_point(mapping = aes(x = EM, y = ECe),alpha=0.5) +
        geom_abline(intercept=ab[1],slope=ab[2])+
        scale_x_continuous(name = "EM",limits=c(0,180)) +
        scale_y_continuous(name = "ECe",limits=c(0,25))

r<-cor(grdUzbekistan$ECe,grdUzbekistan$EM)
```

The correlation coefficient equals `r round(r,3)`. The slope of the fitted line equals `r round(ab[2],4)`. Now a simple random sample of 40 points is selected, and a simple linear model is fitted.

```{r, echo=T}
n<-40
set.seed(314)
ids <- sample.int(nrow(grdUzbekistan), size = n, replace = FALSE)
mysample<-grdUzbekistan[ids,c("ECe","EM")]
#fit simple linear model
lmsample<-lm(ECe~EM,data=mysample)
ab<-coef(lmsample)
```

Figure \@ref(fig:ScatterECevsEM) shows the scatter plot for the sample and the fitted line. 

```{r ScatterECevsEM, echo=F, out.width='70%', fig.asp=.6, fig.cap="Scatterplot and fitted line for simple random sample"}
ggplot(data = mysample) +
        geom_point(mapping = aes(x = EM, y = ECe)) +
        geom_abline(intercept=ab[1],slope=ab[2])+
        scale_x_continuous(name = "EM",limits=c(0,180)) +
        scale_y_continuous(name = "ECe",limits=c(0,25))
```

The simple random sample is used to estimate the population mean of the study variable ECe by the regression estimator, and to approximate the sampling variance of the regression estimator.

```{r}
populationmeanx<-mean(grdUzbekistan$EM) #population mean of covariate
samplemeanx<-mean(mysample$EM) #sample mean of covariate
samplemeanz<-mean(mysample$ECe) #sample mean of study variable
regressionestimate<-samplemeanz+ab[2]*(populationmeanx-samplemeanx) #regression estimate
e<-residuals(lmsample)
S2e<-sum(e^2)/(n-2) #estimated variance of residuals
varregressionestimate<-S2e/n #estimated approximate variance of regression estimator
```

The difference $\delta(x)$ between the population mean of the covariate EM (`r round(populationmeanx,3)`) and its estimated mean (`r round(samplemeanx,3)`) equals `r round(populationmeanx-samplemeanx,3)`. We may expect the difference between the unknown population mean of the study variable ECe and its sample mean (`r round(samplemeanz,3)`) to be equal to $\delta(x)$, multiplied by the estimated slope of the line, which equals `r round(ab[2],4)`. The result (`r round(ab[2]*(populationmeanx-samplemeanx),4)`)is added to the simple random sample estimate. The resulting regression estimate (`r round(regressionestimate,3)`) is closer to the population spatial mean of $z$ (`r round(mean(grdUzbekistan$ECe),3)`) than the simple random sample estimate (`r round(samplemeanz,3)`).

The estimated approximate variance of the regression estimator equals `r round(varregressionestimate,3)`.Note that in approximating the variance of the regression estimator, the variance of the residuals is estimated by dividing the sum of squared residuals by $n-2$, not $n-1$. The reason is that we estimated two regression coefficients, the intercept and slope, so that we loose two degrees of freedom.

Figure @\ref(fig:SamplingDistributionRegression) shows the sampling distribution of the simple regression estimator, obtained by repeating the random sampling and estimation 10,000 times.

```{r SamplingDistributionRegression, echo=F, out.width='70%', fig.asp=.8, fig.cap="Sampling distribution of regression and Horvitz-Thompson estimator for SI of size 40", cache=T}
NUMBER_OF_SAMPLES <- 10000
RegressionEstimator<-VarRegressionEstimator<-HTEstimator<-numeric(length=NUMBER_OF_SAMPLES)

for (i in 1:NUMBER_OF_SAMPLES) {  
  ids <- sample.int(nrow(grdUzbekistan), size = n, replace = FALSE)
  mysample<-grdUzbekistan[ids,c("ECe","EM")]
  lmsample<-lm(ECe~EM,data=mysample)

  RegressionEstimator[i]<-mean(mysample$ECe)+coef(lmsample)[2]*(mean(grdUzbekistan$EM)-mean(mysample$EM))
  VarRegressionEstimator[i]<-(sum(lmsample$residuals^2)/(n-2))/n
  HTEstimator[i]<-mean(mysample$ECe)
}

estimates<-data.frame(RegressionEstimator,HTEstimator)
names(estimates)<-c("Regression","HT")
estimateslf<-melt(estimates)
ggplot(data = estimateslf) +
    geom_boxplot(aes(y=value,x=variable)) +
    geom_hline(yintercept=mean(grdUzbekistan$ECe),colour="red")+
    scale_x_discrete(name = "Estimator") +
    scale_y_continuous(name = "Estimated mean ECe")

bias<-mean(RegressionEstimator)-mean(grdUzbekistan$ECe)
varR <- var(RegressionEstimator)
meanvarR <- mean(VarRegressionEstimator)
gain<-var(HTEstimator)/var(RegressionEstimator)
```

The average of the 10,000 regression estimates equals `r round(mean(RegressionEstimator),3)`. The population mean of the study variable ECe equals `r round(mean(grdUzbekistan$ECe),3)`. so the estimated bias of the regression estimator equals `r round(bias,3)`. The variance of the 10,000 regression estimates equals `r round(varR,3)`, and the average of the 10,000 estimated approximate variances equals `r round(meanvarR,3)`. The gain in precision due to the regression estimator, quantified by the ratio of the variance of the Horvitz-Thompson estimator to the variance of the regression estimator equals `r round(gain,3)`.

#### Exercise ([RegressionEstimator.R](https://github.com/DickBrus/SpatialSamplingwithR/blob/master/Exercises/RegressionEstimator.R)) {-}
1. Write an R script to select a simple random sample without replacement of size 40 from the cotton farm in Uzbekistan (data are ```CottonFarmUzbekistan.RData```). Estimate the mean electrical conductivity (ECe) by the regression estimator using the  electromagnetic measurements (EM) as a covariate    
2. Approximate the sampling variance of the regression estimator  
3. Repeat the sampling 10,000 times in a for-loop, compute the regression estimator for every sample, and compute the variance of the 10,000 regression estimates  
4. Compute the true sampling variance of the Horvitz-Thompson estimator of the same size, and compute the gain in precision achieved by the regression estimator     


### Regression estimator for stratified simple random sampling

With stratified simple random sampling sampling there are two regression estimators, the *separate* and the *combined* regression estimator. In the first estimator the regression estimator for simple random sampling is applied at the level of the strata. This implies that for each stratum separately a regression coefficient $b_h$ is estimated. The regression estimates of the stratum means are then combined by computing the weighted average, usng the relative sizes of the strata as weights:
\begin{equation}
\hat{\bar{z}}_{\text{sepreg}}=\sum_{h=1}^H w_h \hat{\bar{z}}_{\text{regr,}h}
(\#eq:SeparateRegressionEstimator)
\end{equation}
with
\begin{equation}
\hat{\bar{z}}_{\text{regr,}h}= \bar{z}_{s,h}+b_h\left( \bar{x}_h-\bar{x}_{s,h}\right)
(\#eq:RegressionEstimatorStratumMean)
\end{equation}
with $\bar{z}_{s,h}$ and $\bar{x}_{s,h}$ the stratum sample means of the study variable and covariate, respectively, $\bar{x}_h$ the  mean of the covariate in stratum $h$, and $b_h$ the estimated slope coefficient for stratum $h$.

The variance of the seperate regression estimator of the population mean can be estimated by first estimating the variances of the regression estimators of the stratum means using Equation \@ref(eq:VarianceRegressionEstimatorSI), and then combining these variances using Equation \@ref(eq:EstVarMeanSTSI).

The alternative is the combined regression estimator
\begin{equation}
\hat{\bar{z}}_{\text{combreg}}= \hat{\bar{z}}_{\text{STSI}}+b_{\text{STSI}}\left( \bar{x}-\hat{\bar{x}}_{\text{STSI}}\right)
(\#eq:CombinedRegressionEstimator)
\end{equation}
with
\begin{equation}
b_{\text{STSI}} = \frac{\sum_{h=1}^H w_h^2 \widehat{S^2}_h(x,z)/n_h}{\sum_{h=1}^H w_h^2 \widehat{S^2}_h(x)/n_h}
(\#eq:CombinedRegressionCoefficient)
\end{equation}
with $\widehat{S^2}_h(x,z)$ the estimated covariance of the study variable $z$ and the covariate $x$ in stratum $h$:
\begin{equation}
\widehat{S}^2_h(x,z)=\frac{1}{n_h\,(n_h-1)}\sum\limits_{i=1}^{n_h} (z_{hi}-\bar{z}_{s,h})(x_{hi}-\bar{x}_{s,h})
(\#eq:StratumCovariance)
\end{equation}
and  $\widehat{S^2}_h(x)$ the estimated variance of the covariate $x$ in stratum $h$
\begin{equation}
\widehat{S^2}_h(x)=\frac{1}{n_h\,(n_h-1)}\sum\limits_{i=1}^{n_h} (z_{hi}-\bar{z}_{s,h})^2
(\#eq:StratumVarianceCovariate)
\end{equation}
In the combined regression estimator only one regression coefficient $b$ is estimated, which is an estimate of the *population* regression coefficient (the slope of the line in the exhaustive scatterplot). This combined regression estimator is recommendable when the stratum sample sizes are small, so that the estimated regression coefficients per stratum, $b_h$, become unreliable. The estimators above are for infinite populations and for stratified simple random sampling with replacement of finite populations. For sampling without replacement from finite populations, finite population corrections $1-n_h/N_h$ must be added to the numerator and denominator of $b_{\text{STSI}}$, see p. 202 in @coc77.

The variance of the combined regression estimator can be estimated as follows:  
1. estimate the intercept: $a_{\text{STSI}} = \hat{\bar{z}}_{\text{STSI}}-b_{\text{STSI}}\bar{x}$  
2. compute residuals: $e_i = z_i - (a_{\text{STSI}} + b_{\text{STSI}} x_i)$  
3. estimate for each stratum the variance of the estimated mean of the residuals: $\widehat{V}\!\left(\hat{\bar{e}}_h\right)=\widehat{S^{2}}_h(e)/n_h$, with $\widehat{S^{2}}_h(e)$ the estimated variance of the residuals in stratum $h$    
4. combine the estimated variances per stratum: $\widehat{V}\!\left(\hat{\bar{z}}_{\text{combreg}}\right)=\sum_{h=1}^Hw^2_h\widehat{V}\!\left(\hat{\bar{e}}_h\right)$

#### Exercise ([RegressionEstimator_STSI.R](https://github.com/DickBrus/SpatialSamplingwithR/blob/master/Exercises/RegressionEstimator_STSI.R)) {-}
1. Write an R script to estimate the mean ECe of the cotton farm in Uzbekistan by stratified simple random sampling. Construct two strata on the basis of covariate EM, using as a threshold 50. Select a stratified simple random sample without replacement of size 40, and allocate the sample to the strata proportional to their size     
2. Estimate the mean ECe by the seperate regression estimator and approximate the sampling variance of this regression estimator  
3. Estimate the mean ECe by the combined regression estimator and approximate the sampling variance of this regression estimator   

## Ratio estimator
In some cases it is reasonable to assume that the fitted line goes through the origin. An example is the case study on poppy area in Kandahar (Chapter \@ref(pps)). In this case study the covariate is the agricultural area within the  5 km $\times$ 5 km squares that serve as sampling units. Assuming that this covariate is determined without error, it is reasonable to assume that when the covariate equals zero, also the poppy area is zero. This implies that when in fitting a straight line through a scatter plot of poppy area against agricultural area within sampling units, this line must pass the origin. In other words a model without intercept should be fitted. Hereafter it is shown that when the line is fitted with weights inversely proportional to the covariate, the slope of the fitted line equals the ratio of the population total poppy area and the population total agricultural area. 

```{r}
mod<-lm(poppy~agri-1,data=grdKandahar,weights=1/agri)
(slope <- mod$coef)
populationtotalz<-sum(grdKandahar$poppy)
populationtotalx<-sum(grdKandahar$agri)
(ratio<-populationtotalz/populationtotalx)
```

So, if we have a random sample, the slope of the regression line through the origin can simply be estimated by the ratio of the estimated total of the covariate $x$ to the estimated total of the study variable $z$, which is equal to the ratio of the estimated mean of $x$ to the estimated mean of $y$:
\begin{equation}
b=\frac{\hat{t}(z)}{\hat{t}(x)}=\frac{\hat{\bar{z}}}{\hat{\bar{x}}}
(\#eq:RatioOfMeans)
\end{equation}
with $\hat{t}(z)$ and $\hat{t}(x)$ the estimated totals of the study variable and ancillary variable, respectively, and $\hat{\bar{z}}$ and $\hat{\bar{x}}$ the estimated means of the study variable and ancillary variable, respectively. The total of the study variable $z$ can then be estimated by multiplying the total of the covariate $x$ by this ratio:
\begin{equation}
\hat{t}_{\mathrm{ratio}}(z)=b \;t(x)
(\#eq:RatioEstimatorSI)
\end{equation}
with $t(x)$ the total of the ancillary variable, which must be known. This is a general estimator that can be used for any sampling design, not only for simple random sampling. For simple random sampling the population means of $z$ and $x$ are estimated by the sample means.

For simple random sampling the sampling variance of the ratio estimator of the population total can be approximated by
\begin{equation}
\widehat{V}\!\left(\hat{t}_{\mathrm{ratio}}(z)\right)=N^2\frac{\widehat{S^{2}}(e)}{n} \;,
(\#eq:VarianceRatioEstimatorSI)
\end{equation}
with $\widehat{S^{2}}(e)$ the estimated variance of the residuals $e=z-bx$:
\begin{equation}
\widehat{S^{2}}(e)=\frac{1}{(n-1)}\sum\limits_{i=1}^{n}e_i^2
(\#eq:VarianceResidualsRatioEstimator)
\end{equation}
For simple random sampling without replacement from finite populations Equation \@ref(eq:VarianceRatioEstimatorSI) must be multiplied by $(1-\frac{n}{N})$.

```{r SamplingDistributionRatio, echo=F, out.width='70%', fig.asp=.8, fig.cap="Sampling distribution of ratio and Horvitz-Thompson estimator for SI of size 50 from Kandahar",cache=T}
n<-50
N<-nrow(grdKandahar)

#compute total of covariate that will be used in ratio estimator
Tx<-sum(grdKandahar$agri)

RatioEstimator<-VarRatioEstimator<-HTEstimator<-numeric(length=NUMBER_OF_SAMPLES)
set.seed(314)
for (i in 1:NUMBER_OF_SAMPLES) {
  mysample <- sample.int(N, size = n, replace = FALSE)
  b<-mean(grdKandahar$poppy[mysample])/mean(grdKandahar$agri[mysample])
  #now compute ratio estimator
  RatioEstimator[i]<-b*Tx
  #approximate the sampling variance of the ratio estimator
  e<-grdKandahar$poppy[mysample]-b*grdKandahar$agri[mysample]
  VarRatioEstimator[i]<-N^2*(1-(n/N))*var(e)/n
  HTEstimator[i] <- mean(grdKandahar$poppy[mysample])*N
}

estimates<-data.frame(RatioEstimator,HTEstimator)
names(estimates)<-c("Ratio","HT")
estimateslf<-melt(estimates)
ggplot(data = estimateslf) +
    geom_boxplot(aes(y=value,x=variable)) +
    geom_hline(yintercept=mean(grdKandahar$poppy)*N,colour="red")+
    scale_x_discrete(name = "Estimator") +
    scale_y_continuous(name = "Estimated total poppy area")

meanRatio <- mean(RatioEstimator)
bias <- mean(RatioEstimator)-sum(grdKandahar$poppy)
varRatio <- var(RatioEstimator)
meanvarRatio <- mean(VarRatioEstimator)
gain <- var(HTEstimator)/var(RatioEstimator)
```

The average of the 10,000 ratio estimates of the total poppy area equals `r round(meanRatio,0)`. The population total of poppy equals `r round(sum(grdKandahar$poppy),0)`, so the estimated bias of the ratio estimator equals equals`r round(bias,3)`.The variance of the 10,000 ratio estimates equals `r round(varRatio,1)`, and the average of the 10,000 estimated approximate variances equals `r round(meanvarRatio,1)`. The gain in precision due to the ratio estimator, quantified by the ratio of the variance of the Horvitz-Thompson estimator to the variance of the ratio estimator equals `r round(gain,3)`.


### Ratio estimator for stratified simple random sampling

Similar to the regression estimator there are two options: either estimate the ratios separately for the strata, or estimate a combined ratio. The separate ratio estimator is
\begin{equation}
\hat{t}_{\mathrm{ratio,s}}(z)=\sum_{h=1}^H \hat{t}_{h,\mathrm{ratio}}(z)
(\#eq:SeparateRatioEstimatorSTSI)
\end{equation}
with
\begin{equation}
\hat{t}_{h,\mathrm{ratio}}(z)=\frac{\hat{t}_h(z)}{\hat{t}_h(x)} t_h(x)
(\#eq:RatioEstimatorStratumTotal)
\end{equation}
in which $\hat{t}_h(z)$ is the estimated total of the study variable for stratum $h$.

The combined ratio estimator is
\begin{equation}
\hat{t}_{\mathrm{ratio,c}}(z)=\frac{\sum_{h=1}^H\hat{t}_h(z)}{\sum_{h=1}^H\hat{t}_h(x)} t(x)
(\#eq:CombinedRatioEstimatorSTSI)
\end{equation}

## Regression estimator with unknown mean of covariate
The regression estimator \@ref(eq:RegressionEstimatorSI) requires that the population mean of the ancillary variable $x$ is known. This section is about applying the regression estimator in situations where this mean of $x$ is unknown. A possible application is estimating the soil carbon stock (until a given depth) in an area. To estimate this carbon stock soil samples are collected and analyzed in laboratory. These laboratory measurements are very precise, but also expensive. Proximal sensors can be used to derive soil carbon concentrations from the spectra. Compared to laboratory measurements of soil these proximal sensor determinations are much cheaper, but also less precise. If there is a relation between the laboratory and proximal sensing determinations of SOC, then we expect that the regression estimator of the carbon stock will be more precise than the Horvitz-Thompson estimator which does not exploit the proximal sensing measurements. However, the spatial mean of the proximal sensing determinations is unknow. What we can do is estimate this mean from a large sample. At a subsample of this large sample, SOC concentration is also measured in the laboratory. This type of sampling is referred to as *two-phase sampling*. The term `phase' does not refer to a period of time; all data can be collected in one sampling campaign. Intuitively with two-pase sampling the variance of the regression estimator of the total carbon stock will be larger than when the population mean of the proximal sensing determinations would be known. We are more uncertain about the total carbon stock, because we are uncertain about the population mean of the proximal sensing determinations.

Figure \@ref(fig:twophaseCottonFarm) shows a two-phase sample from the cotton farm in Uzbekistan. In the first phase 40 points are selected by simple random sampling. In the second phase a subsample of 20 points (the triangles in the plot) are selected from the 40 points by simple random sampling without replacement. At all 40 points of the first-phase sample the covariate EM is measured, whereas the study variable (ECe) is measured at the 20 subsample points only. This sampling design does not require a full-coverage map of EM, which saves costs of fieldwork.

```{r twophasesample}
#set sample sizes
n1 <- 80
n2 <- 40
ids<-sample.int(nrow(grdUzbekistan),size=n1,replace=FALSE)
mysample <- grdUzbekistan[ids,]
#subsample the selected sample, and observe the study variable
ids2 <- sample.int(nrow(mysample), size = n2, replace = FALSE)
mysubsample<-mysample[ids2,]
```

```{r twophaseCottonFarm, echo=F, out.width='100%', fig.asp=0.5, fig.cap="Two-phase sample for regression estimator of mean ECe"}
ggplot(data = grdUzbekistan) +
  geom_raster(mapping = aes(x = x1/1000, y = x2/1000, fill = EM)) +
  scale_fill_gradientn(name="EM",colours=rev(terrain.colors(10))) +
  geom_point(data=mysample,mapping = aes(x=x1/1000, y = x2/1000), shape=1, size=1.5) +
  geom_point(data=mysubsample,mapping = aes(x=x1/1000, y = x2/1000), shape=2, size=2.5) +
  scale_x_continuous(name="Easting (km)")+
  scale_y_continuous(name="Northing (km)")+
  coord_equal()
```

Estimation of the spatial mean or total by the regression estimator from a two-phase sample is very similar to estimation when the covariate mean is known, as described above. The observations on the *subsample* can be used to estimate the regression coefficients $b$. The true spatial mean of the ancillary variable, ($\bar{x}$), is unknown now. This true mean is replaced by the mean as estimated from the relatively large first-phase sample. The estimated mean $\hat{\bar{x}}_{\mathrm{SI}}$ is estimated from the subsample. This leads to the following estimator
\begin{equation}
\hat{\bar{z}}_{\text{regr,2ph}}= \bar{z}_{s2}+b\left( \bar{x}_{s1}-\bar{x}_{s2}\right) \;,
(\#eq:RegressionEstimatorTwoPhase)
\end{equation}
where $\bar{z}_{s2}$ is the subsample mean of the study variable, and $\bar{x}_{s1}$ and $\bar{x}_{s2}$ are the means of the covariate in the first-phase sample and subsample (second-phase sample), respectively.

The sampling variance is larger than that of the regression estimator with known mean of $x$. The variance can be decomposed into a component equal to the sampling variance of the estimated mean of $z$, as estimated from the first-phase sample (supposing that the study variable would be observed on all units of the first-phase sample), and a component equal to the sampling variance of the regression estimator of the mean of $z$ in the first-phase sample:
\begin{equation}
\widehat{V}\!\left(\hat{\bar{z}}_{\mathrm{regr,2ph}}\right)=\frac{\widehat{S^{2}}(z)}{n_1} + (1-\frac{n_2}{n_1}) \frac{\widehat{S^{2}}(e)}{n_2} \;,
(\#eq:VarianceRegressionEstimatorTwoPhase)
\end{equation}
with $\widehat{S^{2}}(e)$ the variance of the regression residuals as estimated from the subsample
\begin{equation}
\widehat{S^{2}}(e)=\frac{1}{(n_2-2)}\sum\limits_{i=1}^{n}\left(e_{i}\right)^{2}
(\#eq:VarianceResidualsTwoPhase)
\end{equation}
Note the finite population correction (fpc) $(1-n_2/n_1)$ in the variance estimator. This fpc accounts for the reduced variance due to subsampling the first-phase sample *without replacement*.

```{r SamplingDistributionRegression2phase, echo=F, out.width='70%', fig.asp=.8, fig.cap="Sampling distribution of regression estimator for two-phase sampling and regression estimator with mean of covariate known",cache=T}
RegressionEstimator2phase<-VarRegressionEstimator2phase<-numeric(length=NUMBER_OF_SAMPLES)

set.seed(314)

for (i in 1:NUMBER_OF_SAMPLES) {
  #select large simple random sample and observe the ancillary variable
  ids <- sample.int(nrow(grdUzbekistan), size = n1, replace = FALSE)
  mysample<-grdUzbekistan[ids,]
  #now subsample the selected sample and observe the target variable SOM
  subsampleids <- sample.int(nrow(mysample), size = n2, replace = FALSE)
  mysubsample<-mysample[subsampleids,]

  #fit simple linear model
  lmsample<-lm(ECe~EM,data=mysubsample)

  #now compute regression estimator for two-phase sampling
  RegressionEstimator2phase[i]<-mean(mysubsample$ECe)+coef(lmsample)[2]*(mean(mysample$EM)-mean(mysubsample$EM))

  #approximate the sampling variance of the regression estimator
  VarRegressionEstimator2phase[i]<-var(mysubsample$ECe)/n1 + (1-n2/n1)* (sum(lmsample$residuals^2)/(n2-2))/n1
}

estimates<-data.frame(RegressionEstimator2phase,RegressionEstimator)
names(estimates)<-c("Regression_2phase","Regression")
estimateslf<-melt(estimates)
ggplot(data = estimateslf) +
    geom_boxplot(aes(y=value,x=variable)) +
    geom_hline(yintercept=mean(grdUzbekistan$ECe),colour="red")+
    scale_x_discrete(name = "Estimator") +
    scale_y_continuous(name = "Estimated mean ECe")
```

I repeated the two-phase sampling 10,000 times, see Figure \@ref(fig:SamplingDistributionRegression2phase). The variance of the regression estimator equals `r round(var(RegressionEstimator2phase),4)`. This is larger than for the regression estimator with known mean of the covariate EM (`r round(var(RegressionEstimator),4)`), but still smaller than the variance of the Horvitz-Thompson estimator (`r round(var(grdUzbekistan$ECe)/n2,4)`).

#### Exercise ([RegressionEstimator\_Twophase.R](https://github.com/DickBrus/SpatialSamplingwithR/blob/master/Exercises/RegressionEstimator_Twophase.R)) {-}
1. Write an R script to select a simple random sample without replacement of 80 points and a subsample of 40 points by the same sampling design from the cotton farm in Uzbekistan. Estimate the population mean of ECe by the regression estimator for two-phase sampling using EM as a covariate (EM is observed at all 80 points, ECe at the subsample only)    
2. Approximate the sampling variance of the regression estimator  
3. Repeat this 10,000 times in a for-loop, and compute the variance of the 10,000 regression estimates of the mean of ECe  
4. Compare the variance of the regression estimator for this two-phase sampling design with the variance of the regression estimator with known population mean of EM.